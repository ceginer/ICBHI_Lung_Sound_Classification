{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ef6b52e2",
      "metadata": {
        "id": "ef6b52e2"
      },
      "source": [
        "#### 환경설정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dc03a42",
      "metadata": {
        "id": "9dc03a42"
      },
      "source": [
        "##### 1. Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04d7d6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f04d7d6f",
        "outputId": "3c57e712-a314-411b-d4cf-1a953a648946"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# wandb 로그인\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "992382bf",
      "metadata": {
        "id": "992382bf"
      },
      "source": [
        "##### 2. 라이브러리 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5MISAwpScmYt",
      "metadata": {
        "id": "5MISAwpScmYt"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ebed6c5",
      "metadata": {
        "id": "8ebed6c5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torch import Tensor\n",
        "from torchsummary import summary\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d453e5f",
      "metadata": {
        "id": "2d453e5f"
      },
      "source": [
        "##### 3. 경로 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mSXgKx8GoItj",
      "metadata": {
        "id": "mSXgKx8GoItj"
      },
      "outputs": [],
      "source": [
        "ROOT = \"/home/sbw/BOAZ-Chungzins/data/raw\"\n",
        "CHECKPOINT_PATH = \"/home/sbw/boaz/notebook/note_ckp\"\n",
        "PICKLE_PATH = \"/home/sbw/boaz/notebook/pickle\"\n",
        "text = \"/home/sbw/BOAZ-Chungzins/data/metadata/train_test_split.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecaaf5a1",
      "metadata": {
        "id": "ecaaf5a1"
      },
      "source": [
        "##### 4. Seed 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f4e372",
      "metadata": {
        "id": "c9f4e372"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # ✅ 모든 GPU에 동일하게\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # 추가: DataLoader에 worker_init_fn 활용 (아래 예시 참고)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nydfgBckyPt3",
      "metadata": {
        "id": "nydfgBckyPt3"
      },
      "source": [
        "## 1. Data Load"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce3fdcf1",
      "metadata": {
        "id": "ce3fdcf1"
      },
      "source": [
        "#### 1.1 Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OhJa9jivcg1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhJa9jivcg1k",
        "outputId": "a78dee02-13e0-4e6f-cce1-455b6e9da489"
      },
      "outputs": [],
      "source": [
        "# WAV 파일이 있는 디렉토리 경로\n",
        "data_dir = ROOT\n",
        "txt_dir = ROOT\n",
        "\n",
        "df = pd.read_csv(text, sep='\\t', header=None)\n",
        "\n",
        "# 컬럼 이름 변경\n",
        "df.columns = ['filename', 'set']\n",
        "\n",
        "# train, test split\n",
        "train_df = df[df['set'] == 'train']\n",
        "test_df = df[df['set'] == 'test']\n",
        "\n",
        "# filename list\n",
        "train_list = sorted(train_df['filename'].tolist())\n",
        "test_list = sorted(test_df['filename'].tolist())\n",
        "\n",
        "print(f'Train :{len(train_list)}, Test: {len(test_list)}, Total: {len(train_list) + len(test_list)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04291977",
      "metadata": {
        "id": "04291977"
      },
      "source": [
        "#### 1.2 Pretext-Finetune Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ESBIVnKej0G9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESBIVnKej0G9",
        "outputId": "849f490c-dd11-4b23-f1da-0fe6482fb71b"
      },
      "outputs": [],
      "source": [
        "# shuffle train data\n",
        "df_shuffled = train_df.sample(frac=1, random_state=42)\n",
        "\n",
        "# split ratio\n",
        "train_size = int(len(df_shuffled))\n",
        "\n",
        "# pretrain\n",
        "pretrain_df = df_shuffled[:train_size]\n",
        "\n",
        "# filename list (pretext_list -> pretrain list)\n",
        "pretrain_list = sorted(pretrain_df['filename'].tolist())\n",
        "\n",
        "# patient id list\n",
        "pretrain_patient_list = []\n",
        "for filename in pretrain_list:\n",
        "    number = int(filename.split('_')[0])\n",
        "    pretrain_patient_list.append(number)\n",
        "\n",
        "\n",
        "pretrain_patient_counts = pd.Series(pretrain_patient_list).value_counts()\n",
        "\n",
        "print(f\"[Pretrain] 환자 수: {len(pretrain_patient_counts.index)}, 샘플 수: {pretrain_patient_counts.sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oVi6lzuPpSbk",
      "metadata": {
        "id": "oVi6lzuPpSbk"
      },
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8c7719",
      "metadata": {
        "id": "5e8c7719"
      },
      "source": [
        "#### 2.1 Args"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634b232e",
      "metadata": {
        "id": "634b232e"
      },
      "source": [
        "        K: queue size; number of negative keys (default: 65536)\n",
        "        m: moco momentum of updating key encoder (default: 0.999)\n",
        "        T: softmax temperature (default: 0.07)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add5c69b",
      "metadata": {
        "id": "add5c69b"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    # Audio & Spectrogram\n",
        "    target_sr = 16000    # 4KHz\n",
        "    frame_size = 1024\n",
        "    hop_length = 512    # frame_size 절반\n",
        "    n_mels = 64\n",
        "    target_sec = 8\n",
        "\n",
        "    # Augmentation\n",
        "    time_mask_param = 0.5\n",
        "    freq_mask_param = 0.5\n",
        "\n",
        "    # Train\n",
        "    lr = 1e-3 # adamw - 0.03\n",
        "    warm = True                     # warm-up 사용 여부\n",
        "    warm_epochs = 10                # warm-up 적용할 초기 epoch 수\n",
        "    warmup_from = lr * 0.1          # warm-up 시작 learning rate (보통 lr의 10%)\n",
        "    warmup_to = lr\n",
        "\n",
        "    batch_size = 128\n",
        "    workers = 2\n",
        "    epochs = 300\n",
        "    weight_decay = 0.0\n",
        "\n",
        "    resume = None\n",
        "    schedule=[120, 160] # schedule\n",
        "\n",
        "    # MLS\n",
        "    K = 512\n",
        "    momentum = 0.999\n",
        "    T = 0.07\n",
        "    dim_prj = 128\n",
        "    top_k = 20\n",
        "    lambda_bce = 0.3\n",
        "    out_dim = 512\n",
        "\n",
        "    # Linear Evaluation\n",
        "    # ft_epochs = 3\n",
        "\n",
        "    # etc\n",
        "    gpu = 0\n",
        "    data = \"./data_path\"\n",
        "    seed=42\n",
        "    num_classes = 2\n",
        "\n",
        "    # update\n",
        "    ma_update = False\n",
        "    ma_beta = 0.5\n",
        "    target_type = 'grad_flow'\n",
        "    alpha = 0.3\n",
        "\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58e1f949",
      "metadata": {
        "id": "58e1f949"
      },
      "source": [
        "#### 2.2 Utils (func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d2d1329",
      "metadata": {
        "id": "8d2d1329"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# cycle의 클래스를 추출\n",
        "def get_class(cr, wh):\n",
        "    if cr == 1 and wh == 1:\n",
        "        return 3\n",
        "    elif cr == 0 and wh == 1:\n",
        "        return 2\n",
        "    elif cr == 1 and wh == 0:\n",
        "        return 1\n",
        "    elif cr == 0 and wh == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# 256 맟춰주기\n",
        "def generate_mel_spectrogram(waveform, sample_rate, frame_size, hop_length, n_mels):\n",
        "    if hop_length is None:\n",
        "        hop_length = frame_size // 2\n",
        "\n",
        "    mel_spec_transform = T.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_fft=frame_size,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels,\n",
        "        f_min=50,\n",
        "        f_max=2000\n",
        "    )\n",
        "    mel_spectrogram = mel_spec_transform(waveform)\n",
        "    mel_db = T.AmplitudeToDB()(mel_spectrogram)\n",
        "\n",
        "    # dB 스케일에서 매우 낮은 값은 0으로 마스킹\n",
        "    mel_db[mel_db <= -100.0] = 0.0\n",
        "\n",
        "    # 🔧 가운데 padding 적용\n",
        "    target_frames = 256\n",
        "    current_frames = mel_db.shape[-1]\n",
        "    if current_frames < target_frames:\n",
        "        pad_total = target_frames - current_frames\n",
        "        pad_left = pad_total // 2\n",
        "        pad_right = pad_total - pad_left\n",
        "        mel_db = F.pad(mel_db, (pad_left, pad_right))  # center padding\n",
        "    elif current_frames > target_frames:\n",
        "        # 가운데 자르기\n",
        "        start = (current_frames - target_frames) // 2\n",
        "        mel_db = mel_db[:, :, start:start + target_frames]\n",
        "\n",
        "    return mel_db\n",
        "\n",
        "# Cycle Repeat 또는 Crop\n",
        "def repeat_or_truncate_segment(mel_segment, target_frames):\n",
        "    current_frames = mel_segment.shape[-1]\n",
        "    if current_frames >= target_frames:\n",
        "        return mel_segment[:, :, :target_frames]\n",
        "    else:\n",
        "        repeat_ratio = math.ceil(target_frames / current_frames)\n",
        "        mel_segment = mel_segment.repeat(1, 1, repeat_ratio)\n",
        "        return mel_segment[:, :, :target_frames]\n",
        "\n",
        "def preprocess_waveform_segment(waveform, unit_length):\n",
        "\n",
        "    \"\"\"unit_length 기준으로 waveform을 repeat + padding 또는 crop하여 길이 정규화\"\"\"\n",
        "    waveform = waveform.squeeze(0)  # (1, L) → (L,) 로 바꿔도 무방\n",
        "    length_adj = unit_length - len(waveform)\n",
        "\n",
        "    if length_adj > 0:\n",
        "        # waveform이 너무 짧은 경우 → repeat + zero-padding\n",
        "        half_unit = unit_length // 2\n",
        "\n",
        "        if length_adj < half_unit:\n",
        "            # 길이 차이가 작으면 단순 padding\n",
        "            half_adj = length_adj // 2\n",
        "            waveform = F.pad(waveform, (half_adj, length_adj - half_adj))\n",
        "        else:\n",
        "            # 반복 후 부족한 부분 padding\n",
        "            repeat_factor = unit_length // len(waveform)\n",
        "            waveform = waveform.repeat(repeat_factor)[:unit_length]\n",
        "            remaining = unit_length - len(waveform)\n",
        "            half_pad = remaining // 2\n",
        "            waveform = F.pad(waveform, (half_pad, remaining - half_pad))\n",
        "    else:\n",
        "        # waveform이 너무 길면 앞쪽 1/4 내에서 랜덤 crop\n",
        "        length_adj = len(waveform) - unit_length\n",
        "        start = random.randint(0, length_adj // 4)\n",
        "        waveform = waveform[start:start + unit_length]\n",
        "\n",
        "    return waveform.unsqueeze(0)  # 다시 (1, L)로\n",
        "\n",
        "\n",
        "# 데이터 Spec Augmentation ( 0~80% Random Masking )\n",
        "def apply_spec_augment(mel_segment):\n",
        "\n",
        "    M = mel_segment.shape[-1]\n",
        "    F = mel_segment.shape[-2]\n",
        "\n",
        "    # torchaudio의 마스킹은 0부터 mask_param까지 균등분포에서 랜덤하게 길이를 선택\n",
        "    time_masking = T.TimeMasking(time_mask_param=int(M * 0.8))\n",
        "    freq_masking = T.FrequencyMasking(freq_mask_param=int(F * 0.8) )\n",
        "\n",
        "    aug1 = freq_masking(mel_segment.clone())\n",
        "    aug2 = time_masking(mel_segment.clone())\n",
        "    aug3 = freq_masking(time_masking(mel_segment.clone()))\n",
        "\n",
        "    return aug1, aug2, aug3\n",
        "\n",
        "# Waveform resample\n",
        "def resample_waveform(waveform, orig_sr, target_sr=args.target_sr):\n",
        "    if orig_sr != target_sr:\n",
        "        resampler = torchaudio.transforms.Resample(\n",
        "            orig_freq=orig_sr,\n",
        "            new_freq=target_sr\n",
        "        )\n",
        "        return resampler(waveform), target_sr\n",
        "    return waveform, orig_sr\n",
        "\n",
        "# Normalize - Mean/Std\n",
        "# def get_mean_and_std(dataset):\n",
        "#     \"\"\" 전체 mel-spectrogram에서 mean과 std 계산 \"\"\"\n",
        "#     dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "#     cnt = 0\n",
        "#     fst_moment = torch.zeros(1)\n",
        "#     snd_moment = torch.zeros(1)\n",
        "#     for inputs, _, _ in tqdm(dataloader, desc=\"[Calculating Mean/Std]\"):\n",
        "#         b, c, h, w = inputs.shape  # inputs: [1, 1, n_mels, time]\n",
        "#         nb_pixels = b * h * w\n",
        "\n",
        "#         fst_moment += torch.sum(inputs, dim=[0, 2, 3])\n",
        "#         snd_moment += torch.sum(inputs**2, dim=[0, 2, 3])\n",
        "#         cnt += nb_pixels\n",
        "\n",
        "#     mean = fst_moment / cnt\n",
        "#     std = torch.sqrt(snd_moment / cnt - mean**2)\n",
        "#     return mean.item(), std.item()\n",
        "\n",
        "def get_mean_and_std(dataset, mask_threshold=-99.0):\n",
        "    \"\"\" 마스킹(-100 등)을 제외하고 mean/std 계산 \"\"\"\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "    cnt = 0\n",
        "    fst_moment = 0.0\n",
        "    snd_moment = 0.0\n",
        "\n",
        "    for inputs, _, _ in tqdm(dataloader, desc=\"[Calculating Mean/Std]\"):\n",
        "        # mask: 유효한 mel 값만 추출 (e.g. > -99.0)\n",
        "        valid = inputs[inputs > mask_threshold]  # 1D tensor\n",
        "\n",
        "        fst_moment += valid.sum().item()\n",
        "        snd_moment += (valid ** 2).sum().item()\n",
        "        cnt += valid.numel()\n",
        "\n",
        "    mean = fst_moment / cnt\n",
        "    std = np.sqrt(snd_moment / cnt - mean**2)\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a62aa74",
      "metadata": {
        "id": "4a62aa74"
      },
      "outputs": [],
      "source": [
        "def get_timestamp():\n",
        "    \"\"\"Outputs current time in KST like 2404070830\"\"\"\n",
        "    kst_time = datetime.now(ZoneInfo(\"Asia/Seoul\"))\n",
        "    return kst_time.strftime('%y%m%d%H%M')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e684cb",
      "metadata": {
        "id": "39e684cb"
      },
      "source": [
        "#### 2.3 CycleDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1642a79a",
      "metadata": {
        "id": "1642a79a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CycleDataset(Dataset):\n",
        "    def __init__(self, filename_list, wav_dir, txt_dir, target_sec=args.target_sec, target_sr=args.target_sr, frame_size=args.frame_size, hop_length=args.hop_length, n_mels=args.n_mels, mean=None, std=None):\n",
        "        self.filename_list = filename_list\n",
        "        self.wav_dir = wav_dir\n",
        "        self.txt_dir = txt_dir\n",
        "        self.target_sec = target_sec\n",
        "        self.target_sr = target_sr\n",
        "        self.frame_size = frame_size\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mels = n_mels\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "        self.cycle_list = []\n",
        "\n",
        "        print(\"[INFO] Preprocessing cycles...\")\n",
        "        for filename in tqdm(self.filename_list):\n",
        "            txt_path = os.path.join(self.txt_dir, filename + '.txt')\n",
        "            wav_path = os.path.join(self.wav_dir, filename + '.wav')\n",
        "\n",
        "            if not os.path.exists(txt_path):\n",
        "                print(f\"[WARNING] Missing file: {txt_path}\")\n",
        "            if not os.path.exists(wav_path):\n",
        "                print(f\"[WARNING] Missing file: {wav_path}\")\n",
        "\n",
        "            # Load annotation\n",
        "            cycle_data = np.loadtxt(txt_path, usecols=(0, 1))\n",
        "            lung_label = np.loadtxt(txt_path, usecols=(2, 3))\n",
        "\n",
        "            # Load waveform\n",
        "            waveform, orig_sr = torchaudio.load(wav_path)\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)  # Stereo to mono\n",
        "                print(' waveform.shape[0] > 1:')\n",
        "\n",
        "            # Resample to target sample rate (4kHz)\n",
        "            waveform, sample_rate = resample_waveform(waveform, orig_sr, self.target_sr)\n",
        "\n",
        "            for idx in range(len(cycle_data)):\n",
        "                # 호흡 주기 start, end\n",
        "                start_sample = int(cycle_data[idx, 0] * sample_rate)\n",
        "                end_sample = int(cycle_data[idx, 1] * sample_rate)\n",
        "                lung_duration = cycle_data[idx, 1] - cycle_data[idx, 0]\n",
        "\n",
        "                if end_sample <= start_sample:\n",
        "                    print('end_sample <= start_sample:')\n",
        "                    continue  # 잘못된 구간 스킵\n",
        "\n",
        "                # Waveform repeat + padding 후 Mel_db\n",
        "                cycle_wave = waveform[:, start_sample:end_sample]\n",
        "                seg_wave = preprocess_waveform_segment(cycle_wave, unit_length=int(self.target_sec * self.target_sr))\n",
        "                mel = generate_mel_spectrogram(seg_wave, sample_rate, frame_size=self.frame_size, hop_length=self.hop_length, n_mels=self.n_mels)\n",
        "\n",
        "                # 정규화\n",
        "                if self.mean is not None and self.std is not None:\n",
        "                    mask_value = -100.0 # mel db 에서 마스킹된 값\n",
        "                    mask = (mel == mask_value)\n",
        "                    mel = (mel - mean) / std\n",
        "                    mel[mask] = 0.0\n",
        "                    \n",
        "                # crackle, wheeze -> class\n",
        "                cr = int(lung_label[idx, 0])\n",
        "                wh = int(lung_label[idx, 1])\n",
        "                label = get_class(cr, wh)\n",
        "\n",
        "                multi_label = torch.tensor([\n",
        "                    float(label in [1, 3]),\n",
        "                    float(label in [2, 3])\n",
        "                ])  # 변환된 multi-label 반환\n",
        "\n",
        "                # meta_data\n",
        "                meta_data = (filename, lung_duration)\n",
        "\n",
        "                self.cycle_list.append((mel, multi_label, meta_data))\n",
        "\n",
        "        print(f\"[INFO] Total cycles collected: {len(self.cycle_list)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cycle_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mel, label, meta_data = self.cycle_list[idx]\n",
        "        return mel, label, meta_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55d5a070",
      "metadata": {
        "id": "55d5a070"
      },
      "source": [
        "##### Pickle.dump"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752088df",
      "metadata": {
        "id": "752088df"
      },
      "source": [
        "CycleDataset 객체 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67673cde",
      "metadata": {},
      "outputs": [],
      "source": [
        "len(train_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d386e82",
      "metadata": {
        "id": "9d386e82"
      },
      "outputs": [],
      "source": [
        "# # # import random\n",
        "# # # import matplotlib.pyplot as plt\n",
        "# # # import librosa.display\n",
        "\n",
        "# # # wav_dir = ROOT\n",
        "# # # txt_dir = ROOT\n",
        "\n",
        "# # # # 1. Dataset 로드\n",
        "# # # train_dataset = CycleDataset(train_list, wav_dir, txt_dir)\n",
        "# # # test_dataset = CycleDataset(test_list, wav_dir, txt_dir)\n",
        "\n",
        "# # ################################################################\n",
        "\n",
        "# import random\n",
        "# import matplotlib.pyplot as plt\n",
        "# import librosa.display\n",
        "\n",
        "# wav_dir = ROOT\n",
        "# txt_dir = ROOT\n",
        "\n",
        "# # # mean, std 먼저 계산\n",
        "# # normless_dataset = CycleDataset(train_list, wav_dir, txt_dir)\n",
        "# # mean, std = get_mean_and_std(normless_dataset)\n",
        "\n",
        "# # 정규화 적용된 데이터셋 생성\n",
        "# train_dataset = CycleDataset(train_list, wav_dir, txt_dir)\n",
        "# test_dataset = CycleDataset(test_list, wav_dir, txt_dir)\n",
        "\n",
        "# pickle_dict = {\n",
        "#     'train_dataset': train_dataset,\n",
        "#     'test_dataset': test_dataset\n",
        "# }\n",
        "\n",
        "# save_path = os.path.join(PICKLE_PATH, '0721_MLS_datasets.pkl')\n",
        "# with open(save_path, 'wb') as f:\n",
        "#     pickle.dump(pickle_dict, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4BQgVyGnrDbN",
      "metadata": {
        "id": "4BQgVyGnrDbN"
      },
      "source": [
        "pickle로 train_dataset, test_dataset 외부 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a273735",
      "metadata": {
        "id": "7a273735"
      },
      "outputs": [],
      "source": [
        "# pickle_name = f'Aug_Moco_MLS_MelSpec_{args.target_sr//1000}kHz_{args.frame_size}win_{args.hop_length}hop_{args.n_mels}mel_{args.target_sec}s'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd34caa0",
      "metadata": {
        "id": "cd34caa0"
      },
      "outputs": [],
      "source": [
        "# pickle_dict = {\n",
        "#     'train_dataset': train_dataset,\n",
        "#     'test_dataset': test_dataset\n",
        "# }\n",
        "\n",
        "# save_path = os.path.join(PICKLE_PATH, '3:7_saved_datasets_multilabel.pkl')\n",
        "# with open(save_path, 'wb') as f:\n",
        "#     pickle.dump(pickle_dict, f)\n",
        "\n",
        "# # #####\n",
        "\n",
        "# # 🔹 mean, std 함께 저장\n",
        "# pickle_dict = {\n",
        "#     'train_dataset': train_dataset,\n",
        "#     'test_dataset': test_dataset,\n",
        "#     'mean': mean,\n",
        "#     'std': std\n",
        "# }\n",
        "# with open(os.path.join(PICKLE_PATH, 'pad0_norm_saved_datasets_multilabel.pkl'), 'wb') as f:\n",
        "#     pickle.dump(pickle_dict, f)\n",
        "\n",
        "# print(f'mean: {mean}, std: {std}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zRqSwthYTtxq",
      "metadata": {
        "id": "zRqSwthYTtxq"
      },
      "outputs": [],
      "source": [
        "# # 2. 간단 통계\n",
        "# print(f\"Total cycles: {len(train_dataset)}\")\n",
        "\n",
        "# label_counter = [0] * 4  # normal, crackle, wheeze, both\n",
        "# for _, multi_label,_ in train_dataset:\n",
        "#     if torch.equal(multi_label, torch.tensor([0., 0.])):\n",
        "#         label_counter[0] += 1\n",
        "#     elif torch.equal(multi_label, torch.tensor([1., 0.])):\n",
        "#         label_counter[1] += 1\n",
        "#     elif torch.equal(multi_label, torch.tensor([0., 1.])):\n",
        "#         label_counter[2] += 1\n",
        "#     elif torch.equal(multi_label, torch.tensor([1., 1.])):\n",
        "#         label_counter[3] += 1\n",
        "\n",
        "# for idx, count in enumerate(label_counter):\n",
        "#     print(f\"Class {idx}: {count} cycles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yJPtbC3BrqAE",
      "metadata": {
        "id": "yJPtbC3BrqAE"
      },
      "source": [
        "##### Pickle.load\n",
        "저장된 train_dataset, test_dataset을 로드  \n",
        "(> Aug 는 Moco 모델에서 사용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EWrjdCFSrmER",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWrjdCFSrmER",
        "outputId": "ad9770b7-e29b-4c41-e83e-2d35bb552c19"
      },
      "outputs": [],
      "source": [
        "save_path = os.path.join(PICKLE_PATH, 'Mix_MLATT_datasets.pkl')\n",
        "with open(save_path, 'rb') as f:\n",
        "    pickle_dict = pickle.load(f)\n",
        "\n",
        "train_dataset = pickle_dict['train_dataset']\n",
        "test_dataset = pickle_dict['test_dataset']\n",
        "\n",
        "print(f\"[Train] Cycles: {len(train_dataset)}\")\n",
        "print(f\"[Test] Cycles: {len(test_dataset)}\")\n",
        "\n",
        "###################\n",
        "\n",
        "# save_path = os.path.join(PICKLE_PATH, 'pad0_norm_saved_datasets_multilabel.pkl')\n",
        "# # 🔹 load with normalization values\n",
        "# with open(save_path, 'rb') as f:\n",
        "#     pickle_dict = pickle.load(f)\n",
        "\n",
        "# train_dataset = pickle_dict['train_dataset']\n",
        "# test_dataset = pickle_dict['test_dataset']\n",
        "# mean = pickle_dict['mean']\n",
        "# std = pickle_dict['std']\n",
        "\n",
        "# print(f\"[Train] Cycles: {len(train_dataset)}\")\n",
        "# print(f\"[Test] Cycles: {len(test_dataset)}\")\n",
        "# print(f\"[INFO] Loaded mean={mean:.4f}, std={std:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3c24b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cebfa6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 로드\n",
        "mel = train_dataset[0][0]  # (1, 64, 256)\n",
        "\n",
        "# 증강 적용\n",
        "aug_speconly, _ , _ = aug(mel)  # aug1: speconly, aug2: speconly\n",
        "\n",
        "# 시각화 함수\n",
        "def show_mel(mel_tensor, title):\n",
        "    # 텐서 shape이 (1, 64, 256) 또는 (1, 1, 64, 256)일 수 있음\n",
        "    if mel_tensor.ndim == 4:\n",
        "        mel_tensor = mel_tensor.squeeze(0)  # (1, 64, 256)\n",
        "    mel_np = mel_tensor.squeeze(0).cpu().numpy()  # (64, 256)\n",
        "    \n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.imshow(mel_np, origin='lower', aspect='auto', cmap='magma')\n",
        "    plt.colorbar()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Mel Frequency\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 시각화\n",
        "show_mel(mel, \"Original Mel\")\n",
        "show_mel(aug_speconly, \"Augmented Mel (Spec Only)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcca3481",
      "metadata": {
        "id": "bcca3481"
      },
      "source": [
        "#### 2.4 DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f19b4a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f19b4a7",
        "outputId": "c2ac9fb5-3049-4aaa-f5bb-c28a747088d8"
      },
      "outputs": [],
      "source": [
        "# ---------------- 학습 데이터 구성(seed) ----------------\n",
        "seed_everything(args.seed)\n",
        "\n",
        "# train_dataset 내에서 각 파일의 인덱스를 추출\n",
        "pretrain_idx = []\n",
        "\n",
        "for i in range(len(train_dataset)):\n",
        "    filename = train_dataset[i][2][0]\n",
        "\n",
        "    if filename in pretrain_list:\n",
        "        pretrain_idx.append(i)\n",
        "\n",
        "# 인덱스 순서 셔플\n",
        "random.shuffle(pretrain_idx)\n",
        "\n",
        "print(f\"Pretrain set size: {len(pretrain_idx)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce2c3c8",
      "metadata": {
        "id": "cce2c3c8"
      },
      "source": [
        "코드 실행 환경에 따라 num_workers를 적절한 값으로 지정해주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432ae0cd",
      "metadata": {
        "id": "432ae0cd"
      },
      "outputs": [],
      "source": [
        "# Dataset 생성 (Subset)\n",
        "pretrain_dataset = Subset(train_dataset, pretrain_idx)\n",
        "\n",
        "# DataLoader 생성\n",
        "# DataLoader에서 shuffle=True로 지정하면 매 epoch마다 셔플 순서가 달라짐 => 재현성 문제 발생\n",
        "# pretrain_dataset은 이미 셔플이 완료된 것으로, 이것을 DataLoader에 입력함\n",
        "pretrain_loader = DataLoader(\n",
        "    pretrain_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=0,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b492ad67",
      "metadata": {
        "id": "b492ad67"
      },
      "source": [
        "label 분포 확인 (단순 참고용, 실제 환경에서는 pretrain set의 label 분포가 어떤지 알 수 없음)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea9d290",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fea9d290",
        "outputId": "2066f8d9-fbc5-44a6-b3b3-0bc214f136ca"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# label\n",
        "labels = torch.stack([multi_label for _, multi_label, _ in train_dataset])\n",
        "\n",
        "# pretext와 finetune 데이터셋의 라벨 분포 출력\n",
        "pretrain_labels = labels[pretrain_idx]\n",
        "pretrain_labels_class = (\n",
        "    pretrain_labels[:, 0].long() * 1 +  # crackle bit → *1\n",
        "    pretrain_labels[:, 1].long() * 2    # wheeze bit  → *2\n",
        ")  # [N] shape, values in {0, 1, 2, 3}\n",
        "\n",
        "\n",
        "# test 데이터셋의 라벨 분포 출력\n",
        "test_labels = torch.stack([multi_label for _, multi_label, _ in test_dataset])\n",
        "test_labels_class = (\n",
        "    test_labels[:, 0].long() * 1 +  # crackle bit → *1\n",
        "    test_labels[:, 1].long() * 2    # wheeze bit  → *2\n",
        ")  # [N] shape, values in {0, 1, 2, 3}\n",
        "\n",
        "print(f\"Pretrain sample: {len(pretrain_labels_class)}\")\n",
        "print(\"Pretrain label distribution:\", Counter(pretrain_labels_class.tolist()))\n",
        "print(f\"Test sample: {len(test_labels_class)}\")\n",
        "print(\"Test label distribution:\", Counter(test_labels_class.tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed559ff1",
      "metadata": {
        "id": "ed559ff1"
      },
      "source": [
        "## 3. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca710799",
      "metadata": {
        "id": "ca710799"
      },
      "source": [
        "#### 3.1 Pre-trained ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad728509",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "class ResNet50(torchvision.models.resnet.ResNet):\n",
        "    def __init__(self, track_bn=True):\n",
        "        def norm_layer(*args, **kwargs):\n",
        "            return nn.BatchNorm2d(*args, **kwargs, track_running_stats=track_bn)\n",
        "        super().__init__(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], norm_layer=norm_layer)\n",
        "        del self.fc\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.final_feat_dim = 2048\n",
        "\n",
        "    def load_sl_official_weights(self, progress=True):\n",
        "        weights = ResNet50_Weights.DEFAULT\n",
        "        state_dict = weights.get_state_dict(progress=progress)\n",
        "\n",
        "        del state_dict['conv1.weight']\n",
        "        missing, unexpected = self.load_state_dict(state_dict, strict=False)\n",
        "        # if len(missing) > 0:\n",
        "            # raise AssertionError('Model code may be incorrect')\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76dbd0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def backbone_resnet50_patch():\n",
        "\n",
        "    model = ResNet50()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d7776d9",
      "metadata": {},
      "source": [
        "#### 3.2 Pre-trained CNN6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0de338c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def init_layer(layer):\n",
        "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
        "    nn.init.xavier_uniform_(layer.weight)\n",
        "    if hasattr(layer, 'bias'):\n",
        "        if layer.bias is not None:\n",
        "            layer.bias.data.fill_(0.)\n",
        "            \n",
        "\n",
        "def init_bn(bn):\n",
        "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
        "    bn.bias.data.fill_(0.)\n",
        "    bn.weight.data.fill_(1.)\n",
        "\n",
        "\n",
        "class ConvBlock5x5(nn.Module): #for CNN6\n",
        "    def __init__(self, in_channels, out_channels, stride=(1,1)):\n",
        "        \n",
        "        super(ConvBlock5x5, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
        "                              out_channels=out_channels,\n",
        "                              kernel_size=(5, 5), stride=stride,\n",
        "                              padding=(2, 2), bias=False)\n",
        "                              \n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.init_weight()\n",
        "        \n",
        "    def init_weight(self):\n",
        "        init_layer(self.conv1)\n",
        "        init_bn(self.bn1)\n",
        "        \n",
        "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):        \n",
        "        x = input\n",
        "        x = F.relu_(self.bn1(self.conv1(x)))\n",
        "        if pool_type == 'max':\n",
        "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == 'avg':\n",
        "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == 'avg+max':\n",
        "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
        "            x = x1 + x2\n",
        "        else:\n",
        "            raise Exception('Incorrect argument!')\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class CNN6(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN6, self).__init__()\n",
        "        self.final_feat_dim = 512\n",
        "\n",
        "        self.do_dropout = False\n",
        "        self.conv_block1 = ConvBlock5x5(in_channels=1, out_channels=64, stride=(1,1))\n",
        "        self.conv_block2 = ConvBlock5x5(in_channels=64, out_channels=128, stride=(1,1))\n",
        "        self.conv_block3 = ConvBlock5x5(in_channels=128, out_channels=256, stride=(1,1))\n",
        "        self.conv_block4 = ConvBlock5x5(in_channels=256, out_channels=512, stride=(1,1))\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        # self.linear = nn.Linear(512, num_classes, bias=True)\n",
        "\n",
        "    def load_sl_official_weights(self):\n",
        "        \"\"\" download AudioSet pretrained CNN6 in https://zenodo.org/record/3960586#.Y8dz8y_kEiY\n",
        "        \"\"\"\n",
        "        weights = torch.load('/home/sbw/boaz/notebook/Cnn6_mAP=0.343.pth')['model']\n",
        "        state_dict = {k: v for k, v in weights.items() if k in self.state_dict().keys()}\n",
        "        missing, unexpected = self.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    def forward(self, x, return_feature_map=False):\n",
        "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
        "        if self.do_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
        "        if self.do_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
        "        if self.do_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
        "        if self.do_dropout:\n",
        "            x = self.dropout(x)\n",
        "        \n",
        "        if return_feature_map:\n",
        "            return x  # shape: (B, 512, 4, 16)\n",
        "\n",
        "        x = torch.mean(x, dim=3) #mean over time dim\n",
        "        (x1, _) = torch.max(x, dim=2) #max over freq dim\n",
        "        x2 = torch.mean(x, dim=2) #mean over freq dim (after mean over time)\n",
        "        x = x1 + x2\n",
        "\n",
        "        # if self.embed_only:\n",
        "        #     return x\n",
        "        # return self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f839c412",
      "metadata": {},
      "outputs": [],
      "source": [
        "def backbone_cnn6():\n",
        "    \"\"\"\n",
        "    MoCo 구조에 사용할 CNN6 백본 정의 함수.\n",
        "    \n",
        "    주요 변경 사항:\n",
        "    - ResNet50 대신 CNN6 클래스 사용\n",
        "    - 출력 feature dim은 512로 고정됨 (MoCo에서는 dim_enc=2048 → 이 부분만 맞춰서 사용하면 문제 없음)\n",
        "    - ImageNet pretrained 사용 대신 공식 CNN6 pretrained 로딩 함수 포함 (옵션 사용 가능)\n",
        "    \"\"\"\n",
        "    model = CNN6()\n",
        "\n",
        "    # 공식 SL pretrained weight를 사용하고자 할 경우 아래 줄을 주석 해제\n",
        "    model.load_sl_official_weights()\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89cdc7f9",
      "metadata": {},
      "source": [
        "##### 3.3 Multilabel Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8f81d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# class MultilabelAttention(nn.Module):\n",
        "#     def __init__(self, backbone, num_classes=2, lambda_attn=0.5, attention_heads=[1, float('inf')]):\n",
        "#         super(MultilabelAttention, self).__init__()\n",
        "#         self.backbone = backbone()\n",
        "#         self.num_classes = num_classes\n",
        "#         self.lambda_attn = lambda_attn\n",
        "#         self.attention_heads = attention_heads  # e.g., [1, inf] for H=2\n",
        "\n",
        "#         self.class_weights = nn.Parameter(torch.randn(len(attention_heads), num_classes, 512))\n",
        "\n",
        "#         self.output_layer = nn.ModuleList([\n",
        "#             nn.Linear(512, 1) for _ in range(num_classes)\n",
        "#         ])\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # CNN6 백본 통과 → shape: (B, 512, 4, 16)\n",
        "#         feat_map = self.backbone(x, return_feature_map=True)  # (B, 512, 4, 16)\n",
        "\n",
        "#         B, C, Freq, Time = feat_map.shape\n",
        "#         flat_feat = feat_map.view(B, C, Freq * Time).permute(0, 2, 1)  # (B, 64, 512)\n",
        "\n",
        "#         # Class-specific attention a_i 계산\n",
        "#         attn_outputs = []\n",
        "#         for h, T in enumerate(self.attention_heads):\n",
        "#             Ci = self.class_weights[h]  # (num_classes, 512)\n",
        "#             logits = torch.einsum(\"bnc, kc -> bnk\", flat_feat, Ci)  # (B, 64, num_classes)\n",
        "#             logits = logits.permute(0, 2, 1)  # (B, num_classes, 64)\n",
        "#             if T == float('inf'):\n",
        "#                 attn_scores = F.one_hot(torch.argmax(logits, dim=2), num_classes=logits.shape[2]).float()\n",
        "#             else:\n",
        "#                 attn_scores = F.softmax(T * logits, dim=2)  # (B, num_classes, 64)\n",
        "\n",
        "#             attn_scores = attn_scores.unsqueeze(-1)  # (B, num_classes, 64, 1)\n",
        "#             flat_feat_exp = flat_feat.unsqueeze(1)  # (B, 1, 64, 512)\n",
        "#             attn_feat = torch.sum(attn_scores * flat_feat_exp, dim=2)  # (B, num_classes, 512)\n",
        "#             attn_outputs.append(attn_feat)\n",
        "\n",
        "#         # Class-specific global feature g_i 계산\n",
        "#         feat_avg_t = torch.mean(feat_map, dim=3)  # (B, 512, Freq)\n",
        "#         gmp = torch.max(feat_avg_t, dim=2)[0]     # (B, 512)\n",
        "#         gap = torch.mean(feat_avg_t, dim=2)       # (B, 512)\n",
        "#         g = gmp + gap                             # (B, 512)\n",
        "#         g = g.unsqueeze(1).repeat(1, self.num_classes, 1)  # (B, num_classes, 512)\n",
        "\n",
        "#         # Combine: f_i = g_i + lambda * a_i\n",
        "#         combined = g\n",
        "#         for attn in attn_outputs:\n",
        "#             combined = combined + self.lambda_attn * attn  # sum over heads\n",
        "\n",
        "#         # Output layer for each class\n",
        "#         out = []\n",
        "#         for i in range(self.num_classes):\n",
        "#             cls_feat = combined[:, i, :]  # (B, 512)\n",
        "#             logit = self.output_layer[i](cls_feat).squeeze(-1)  # (B,)\n",
        "#             out.append(logit)\n",
        "\n",
        "#         logits = torch.stack(out, dim=1)  # (B, num_classes)\n",
        "#         probs = torch.sigmoid(logits)    # (B, num_classes)\n",
        "#         return combined, logits, probs                 # 마지막 dim: (B, 2, 512) \n",
        "\n",
        "\n",
        "# def backbone_mlattention():\n",
        "#     \"\"\"\n",
        "#     Multi-label attention 기반 backbone 정의 함수\n",
        "#     CNN6 기반 특징 추출기 + CSRA 기반 attention 구조 결합\n",
        "    \n",
        "#     Returns:\n",
        "#         nn.Module: Multi-label attention 기반 분류기\n",
        "#     \"\"\"\n",
        "#     return MultilabelAttention(backbone=backbone_cnn6, num_classes=2, lambda_attn=0.5, attention_heads=[1, float('inf')])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "895f8785",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn(10, 1, 64, 256) # B=10\n",
        "model = backbone_mlattention()\n",
        "out = model(x)  # (B, 2, 512)\n",
        "\n",
        "print(f\"\\ntorch.Size : {out.shape}\")  # → torch.Size([B=10, 2, 512])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a796d30",
      "metadata": {},
      "source": [
        "##### 3.4 Mix-MultiLabel Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8515e944",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def group_mix(group_spec, labels, beta=1.0):\n",
        "    \"\"\"\n",
        "    Refer by CycleGuradian - https://github.com/chumingqian/CycleGuardian/blob/main/nets/CycleGuardian_v5_1_3.py\n",
        "    ( def group_mix )\n",
        "    \"\"\"\n",
        "    B, N, D = group_spec.shape  # B: 배치 크기, N: 그룹 수 (ex. 64), D: 차원 수 (ex. 512)\n",
        "                                # e.g., group_spec.shape == [B, 64, 512]\n",
        "\n",
        "    device = group_spec.device  # e.g., 'cuda:0'\n",
        "\n",
        "    # 🔹 lam: beta 분포에서 샘플링 (mix 비율)\n",
        "    lam = np.random.beta(beta, beta)  # scalar float (e.g., 0.66)\n",
        "\n",
        "    # 🔹 num_mask: 총 N 그룹 중에서 몇 개를 섞을지 결정\n",
        "    num_mask = int(D * (1. - lam))  # scalar int (e.g., 64 * 0.34 = 21)\n",
        "\n",
        "    # 🔹 mask: 섞을 group index (공통)\n",
        "    mask = torch.randperm(D)[:num_mask].to(device)  # shape: [num_mask] (e.g., [21])\n",
        "\n",
        "    # 🔹 index: 다른 sample과 섞기 위해 순서를 섞음\n",
        "    index = torch.randperm(B).to(device)  # shape: [B] (e.g., [3, 0, 1, 2])\n",
        "\n",
        "    # 🔹 mix: 같은 위치의 group들을 index 기준으로 섞기\n",
        "    mixed_group_spec = group_spec.clone()                  # shape: [B, 64, 512]\n",
        "    mixed_group_spec[:, :, mask] = group_spec[index][:, :, mask]  \n",
        "    # group_spec[index]: shape [B, 64, 512]\n",
        "    # group_spec[index][:, mask, :]: shape [B, num_mask, 512]\n",
        "    # 최종적으로 mixed_group_spec[:, mask, :]: shape [B, num_mask, 512]\n",
        "\n",
        "    # 🔹 lam_tensor: 각 sample에 대해 lam 값을 broadcasting 하기 위한 텐서\n",
        "    lam_tensor = torch.full((B,), lam, device=device)  # shape: [B] (e.g., [0.66, 0.66, 0.66, 0.66])\n",
        "\n",
        "    # 🔹 return: 섞은 group, 원래 라벨, 섞인 라벨, lam 값, 섞인 index\n",
        "    return mixed_group_spec, labels, labels[index], lam_tensor, index\n",
        "    # mixed_group_spec: shape [B, 64, 512]\n",
        "    # labels: shape [B] 또는 [B, C] (멀티클래스인지 멀티라벨인지에 따라 다름)\n",
        "    # labels[index]: shape [B] 또는 [B, C]\n",
        "    # lam_tensor: shape [B]\n",
        "    # index: shape [B]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a00d8660",
      "metadata": {},
      "outputs": [],
      "source": [
        "class GroupMixConLoss(torch.nn.Module):\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, proj_orig, proj_mix, labels_a, labels_b, lam, index):\n",
        "        \"\"\"\n",
        "        lam:       [B]             # 각 샘플마다 lam 값\n",
        "        index:     [B]             # 섞인 대상 인덱스\n",
        "        \n",
        "        Refer by CycleGuradian - https://github.com/chumingqian/CycleGuardian/blob/main/nets/CycleGuardian_v5_1_3.py\n",
        "        (class GroupMixConLoss)\n",
        "        \n",
        "        \"\"\"\n",
        "        B = proj_orig.size(0)           # 배치 크기\n",
        "        device = proj_orig.device\n",
        "\n",
        "        # 🔹 L2 정규화\n",
        "        proj_orig = F.normalize(proj_orig, dim=1)   # [B, D]\n",
        "        proj_mix  = F.normalize(proj_mix, dim=1)    # [B, D]\n",
        "        # print(f\"proj_orig.shape: {proj_orig.shape}, proj_mix.shape: {proj_mix.shape}\")\n",
        "\n",
        "        # 🔹 유사도 행렬: mix vs. original 간의 내적\n",
        "        sim_matrix = torch.matmul(proj_mix, proj_orig.T) / self.temperature  \n",
        "        # [B, D] x [D, B] -> [B, B]\n",
        "\n",
        "        # 🔹 마스크 A: 원래 자기 자신이랑만 1인 마스크\n",
        "        mask_a = torch.eye(B, device=device)  # [B, B]\n",
        "\n",
        "        # 🔹 마스크 B: 각 mix가 섞인 대상과 1인 마스크\n",
        "        mask_b = torch.zeros_like(mask_a)     # [B, B]\n",
        "        mask_b[torch.arange(B), index] = 1    # 예: i-th row에서 index[i] column에 1\n",
        "\n",
        "        # 🔹 soft positive mask = lam * identity + (1 - lam) * mix_target\n",
        "        # lam: [B] -> [B, 1], broadcasting 됨\n",
        "        mask = lam.view(-1, 1) * mask_a + (1 - lam).view(-1, 1) * mask_b  # [B, B]\n",
        "\n",
        "        # 수치 안정성 위해 max 빼고 logits 계산\n",
        "        logits_max, _ = torch.max(sim_matrix, dim=1, keepdim=True)\n",
        "        logits = sim_matrix - logits_max.detach()\n",
        "\n",
        "        exp_logits = torch.exp(logits)\n",
        "\n",
        "        # 🔹 softmax log-prob 계산\n",
        "        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True))  \n",
        "        # log_softmax(sim_matrix, dim=1)과 동일\n",
        "        # sim_matrix: [B, B], log_prob: [B, B]\n",
        "\n",
        "        # 🔹 positive log-prob만 평균냄\n",
        "        # 각 row (i)에서 soft positive에 해당하는 위치에 대해서만 log_prob * mask\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(dim=1) / mask.sum(dim=1)  # [B]\n",
        "\n",
        "        # 🔹 전체 평균 loss\n",
        "        loss = -mean_log_prob_pos.mean()  # scalar\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09304124",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MixMLATT(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=2, lambda_attn=0.5, attention_heads=[1, float('inf')], projector_dim=128):\n",
        "        super(MixMLATT, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.lambda_attn = lambda_attn\n",
        "        self.attention_heads = attention_heads\n",
        "\n",
        "        # CNN6 백본 (e.g., CNN6 → [B, 512, 4, 16])\n",
        "        self.backbone = backbone()\n",
        "\n",
        "        # Class-specific Attention weights: 각 head마다 [num_classes, 512]\n",
        "        self.class_weights = nn.Parameter(torch.randn(len(attention_heads), num_classes, 512))\n",
        "\n",
        "    def forward(self, x, mix_feature=False, patch_mix=False, y=None, lam=None, index=None):\n",
        "        \"\"\"\n",
        "        Refer Multi-label class-specific method - https://arxiv.org/abs/2407.10828\n",
        "        Refer CSRA - https://github.com/Kevinz-code/CSRA/blob/master/pipeline/csra.py\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        origin_feat = None\n",
        "\n",
        "        # 1. CNN6 Backbone → [B, 512, 4, 16]\n",
        "        feat_map = self.backbone(x, return_feature_map=True)  # (B, 512, 4, 16)\n",
        "\n",
        "        # 2. Reshape → [B, 64, 512]\n",
        "        feat_flat = feat_map.view(B, 512, -1).permute(0, 2, 1)  # (B, 64, 512)\n",
        "\n",
        "        # 3. Optional: Patch-wise Mixing\n",
        "        if patch_mix and y is not None:\n",
        "            # 1. group_mix 수행\n",
        "            # mixed_group_spec, labels, labels[index], lam_tensor, index\n",
        "            feat_flat, label_origin, label_mix, lam, index = group_mix(feat_flat, y)\n",
        "\n",
        "            # 2. origin_feat 저장\n",
        "            origin_feat = feat_flat.detach() if mix_feature else None\n",
        "\n",
        "        # 4. Class-specific Attention\n",
        "        attn_outputs = []\n",
        "        for h, T in enumerate(self.attention_heads):\n",
        "            class_weight = self.class_weights[h]  # (num_classes, 512)\n",
        "            logits = torch.einsum(\"bnc,kc->bnk\", feat_flat, class_weight)  # (B, 64, num_classes)\n",
        "            logits = logits.permute(0, 2, 1)  # (B, num_classes, 64)\n",
        "\n",
        "            if T == float('inf'):\n",
        "                attn_scores = F.one_hot(torch.argmax(logits, dim=2), num_classes=logits.shape[2]).float()\n",
        "            else:\n",
        "                attn_scores = F.softmax(T * logits, dim=2)\n",
        "\n",
        "            attn_scores = attn_scores.unsqueeze(-1)           # (B, num_classes, 64, 1)\n",
        "            feat_exp = feat_flat.unsqueeze(1)                 # (B, 1, 64, 512)\n",
        "            attn_feat = torch.sum(attn_scores * feat_exp, dim=2)  # (B, num_classes, 512)\n",
        "            attn_outputs.append(attn_feat)\n",
        "\n",
        "        # 5. Global Feature Aggregation\n",
        "        feat_avg_t = torch.mean(feat_map, dim=3)  # (B, 512, Freq)\n",
        "        gmp = torch.max(feat_avg_t, dim=2)[0]     # (B, 512)\n",
        "        gap = torch.mean(feat_avg_t, dim=2)       # (B, 512)\n",
        "        g = gmp + gap                             # (B, 512)\n",
        "        g = g.unsqueeze(1).repeat(1, self.num_classes, 1)  # (B, num_classes, 512)\n",
        "\n",
        "        # 6. Combine Global + Attention Feature\n",
        "        attn_feat = g\n",
        "        for attn in attn_outputs:\n",
        "            attn_feat = attn_feat + self.lambda_attn * attn  # (B, num_classes, 512)\n",
        "\n",
        "        # 8. Return\n",
        "        if not patch_mix:\n",
        "            return attn_feat, origin_feat # [B, 2, 512], [B, 64, 512]\n",
        "        else:\n",
        "            return attn_feat, origin_feat, label_origin, label_mix, lam, index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1648eb8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def backbone_mixmlatt():\n",
        "    \"\"\"\n",
        "    Multi-label attention 기반 backbone 정의 함수\n",
        "    CNN6 기반 특징 추출기 + CSRA 기반 attention 구조 결합\n",
        "    \n",
        "    Returns:\n",
        "        nn.Module: Multi-label attention 기반 분류기\n",
        "    \"\"\"\n",
        "    return MixMLATT(backbone=backbone_cnn6, num_classes=2, lambda_attn=0.5, attention_heads=[1, float('inf')])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09f21daa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09f21daa",
        "outputId": "0c812f61-67a5-474c-a91e-2ed67a2c0c78"
      },
      "outputs": [],
      "source": [
        "# summary 함수 사용: (채널, 높이, 너비) 크기를 지정\n",
        "summary(backbone_mixmlatt().to(device), input_size=(1, 64, 256))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc1bf21",
      "metadata": {
        "id": "1cc1bf21"
      },
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-BkAfkqhyHrY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BkAfkqhyHrY",
        "outputId": "70fa83d7-4e71-4275-eb00-1531ed151726"
      },
      "outputs": [],
      "source": [
        "next(iter(pretrain_loader))[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5056744",
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate(model, classifier, projector_0, projector_1, val_loader, criterion, device, args):\n",
        "    \"\"\"\n",
        "    Multi-label + GroupMix Contrastive 평가용 검증 함수\n",
        "    - args.target_type 에 따라 grad_block, grad_flow, etc. 처리\n",
        "    - 입력: inputs [B, 1, F, T], labels [B, 2]\n",
        "    - 출력: 평균 loss, 전체 label, 전체 예측값\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    projector_0.eval()\n",
        "    projector_1.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, _ in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # 1. Original forward (patch_mix=False)\n",
        "            attn_feat, _ = model(inputs, mix_feature=True, patch_mix=False)  # [B, 2, 512]\n",
        "\n",
        "            # 2. Classification logits\n",
        "            out = []\n",
        "            for i in range(attn_feat.shape[1]):\n",
        "                cls_feat = attn_feat[:, i, :]               # [B, 512]\n",
        "                logit = classifier[i](cls_feat).squeeze(-1) # [B]\n",
        "                out.append(logit)\n",
        "            logits = torch.stack(out, dim=1)  # [B, 2]\n",
        "\n",
        "            # 3. classification loss\n",
        "            loss_ce = criterion[0](logits, labels)\n",
        "\n",
        "            # 4. Projector1 (target type 설정)\n",
        "            if args.target_type == 'grad_block':\n",
        "                proj1_0 = deepcopy(attn_feat[:, 0, :].detach())\n",
        "                proj1_1 = deepcopy(attn_feat[:, 1, :].detach())\n",
        "            elif args.target_type == 'grad_flow':\n",
        "                proj1_0 = attn_feat[:, 0, :]\n",
        "                proj1_1 = attn_feat[:, 1, :]\n",
        "            elif args.target_type == 'project_block':\n",
        "                proj1_0 = projector_0(attn_feat[:, 0, :]).detach()\n",
        "                proj1_1 = projector_1(attn_feat[:, 1, :]).detach()\n",
        "            elif args.target_type == 'project_flow':\n",
        "                proj1_0 = projector_0(attn_feat[:, 0, :])\n",
        "                proj1_1 = projector_1(attn_feat[:, 1, :])\n",
        "\n",
        "            # 5. PatchMix 적용 (mix_feature=True)\n",
        "            mix_attn_feat, origin_feat, label_origin, label_mix, lam, index = model(inputs, y=labels, patch_mix=True, mix_feature=True)\n",
        "\n",
        "            # 6. mix는 무조건 projector 통과\n",
        "            proj2_0 = projector_0(mix_attn_feat[:, 0, :])\n",
        "            proj2_1 = projector_1(mix_attn_feat[:, 1, :])\n",
        "\n",
        "            # 7. Contrastive loss\n",
        "            loss_con0 = criterion[1](proj1_0, proj2_0, label_origin, label_mix, lam, index)\n",
        "            loss_con1 = criterion[1](proj1_1, proj2_1, label_origin, label_mix, lam, index)\n",
        "\n",
        "            # 8. Total loss\n",
        "            loss = loss_ce + args.alpha * (loss_con0 + loss_con1)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # 9. Prediction\n",
        "            preds = (torch.sigmoid(logits) > 0.4).int()\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0).numpy()   # [N, 2]\n",
        "    all_labels = torch.cat(all_labels, dim=0).numpy() # [N, 2]\n",
        "\n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "    return avg_loss, all_labels, all_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69de32ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from copy import deepcopy\n",
        "from torch.cuda.amp import GradScaler\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch\n",
        "from torch.cuda.amp import GradScaler\n",
        "\n",
        "\"\"\"\n",
        "Refered by PatchMix - https://github.com/raymin0223/patch-mix_contrastive_learning/blob/main/main.py\n",
        "(def train.py)\n",
        "\"\"\"\n",
        "\n",
        "# from utils.meters import AverageMeter\n",
        "################################\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_icbhi_scores = []\n",
        "test_icbhi_scores = []\n",
        "test_labels_all = []\n",
        "test_preds_all = []\n",
        "epochs = []\n",
        "\n",
        "# 모델 지정하기 전 seed 고정 필요\n",
        "seed_everything(args.seed) # Seed 고정\n",
        "\n",
        "pretrain_project_name = f'Raw_MultilabelAtt_T_{args.batch_size}bs_{get_timestamp()}'\n",
        "\n",
        "# -------------------------------------------wan\n",
        "# wandb 초기화 (프로젝트명, 실험 이름 등 설정)\n",
        "wandb.init(\n",
        "    project=\"SBW_ICBHI_MLATT_all\", # 프로젝트 이름\n",
        "    name=f\"{pretrain_project_name}\",  # 실험 이름\n",
        "    config={\n",
        "        \"epochs\": args.epochs,\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"lr\": args.lr,\n",
        "        \"momentum\": args.momentum,\n",
        "        \"weight_decay\": args.weight_decay\n",
        "    }\n",
        ")\n",
        "# -------------------------------------------wan\n",
        "\n",
        "################################\n",
        "# 1. Model / Classifier \n",
        "model = MixMLATT(backbone=backbone_cnn6, \n",
        "                 num_classes=2, \n",
        "                 lambda_attn=0.5, \n",
        "                 attention_heads=[1, float('inf')]\n",
        "                 ).cuda()\n",
        "\n",
        "classifier = nn.ModuleList([nn.Linear(args.out_dim, 1) for _ in range(2)]).cuda()\n",
        "\n",
        "# 2. Projector 0/1\n",
        "projector_0 = nn.Sequential(nn.Linear(args.out_dim, args.out_dim),nn.ReLU(),nn.Linear(args.out_dim, args.dim_prj)).cuda()\n",
        "projector_1 = nn.Sequential(nn.Linear(args.out_dim, args.out_dim),nn.ReLU(),nn.Linear(args.out_dim, args.dim_prj)).cuda()\n",
        "\n",
        "# 3. EMA 선언\n",
        "ema_model = deepcopy(model)\n",
        "ema_projector_0 = deepcopy(projector_0)\n",
        "ema_projector_1 = deepcopy(projector_1)\n",
        "ema_classifier = deepcopy(classifier)\n",
        "for m in [ema_model, ema_projector_0, ema_projector_1, ema_classifier]:\n",
        "    m.eval()\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "# 4. criterion\n",
        "criterion = [\n",
        "    nn.BCEWithLogitsLoss().cuda(),       # criterion[0]: classification\n",
        "    GroupMixConLoss(temperature=0.07).cuda()  # criterion[1]: contrastive\n",
        "]\n",
        "\n",
        "# 5. optimizer  \n",
        "optimizer = optim.Adam(\n",
        "    list(model.parameters()) + list(classifier.parameters()) + \n",
        "    list(projector_0.parameters()) + list(projector_1.parameters()),\n",
        "    lr=args.lr, weight_decay=args.weight_decay\n",
        ")\n",
        "\n",
        "\n",
        "# 6. EMA (Exponential Moving Average) 설정\n",
        "@torch.no_grad()\n",
        "def update_ema(student, ema, beta=0.999):\n",
        "    for param, ema_param in zip(student.parameters(), ema.parameters()):\n",
        "        ema_param.data = beta * ema_param.data + (1 - beta) * param.data\n",
        "\n",
        "\n",
        "# 7. Train\n",
        "# Best loss 초기화\n",
        "best_loss = float('inf')\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    # ===============================\n",
        "    # Training\n",
        "    # ===============================\n",
        "    model.train()\n",
        "    projector_0.train()\n",
        "    projector_1.train()\n",
        "    classifier.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    total_predictions = 0.0\n",
        "    correct_predictions = 0.0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    pbar = tqdm(pretrain_loader, desc='Mix_MLATT Trainig only')\n",
        "    for idx, (repeat_mel, labels, _) in enumerate(pretrain_loader):\n",
        "\n",
        "        repeat_mel = repeat_mel.cuda(non_blocking=True)\n",
        "        labels = labels.cuda(non_blocking=True)\n",
        "        bsz = labels.size(0)\n",
        "\n",
        "        # (1) Original 이미지 forward\n",
        "        # attn_feat: [B, 2, 512], origin_feat: [B, 64, 512]\n",
        "        attn_feat, _ = model(repeat_mel, mix_feature=True, patch_mix=False)\n",
        "        out = []\n",
        "        for i in range(args.num_classes):\n",
        "            cls_feat = attn_feat[:, i, :]  # (B, 512)\n",
        "            logit = classifier[i](cls_feat).squeeze(-1)  # (B,)\n",
        "            out.append(logit)\n",
        "        logits = torch.stack(out, dim=1) # [B, 2]\n",
        "\n",
        "        # (2) classification loss\n",
        "        loss_ce = criterion[0](logits, labels)\n",
        "\n",
        "        # (3) projector1 생성 (class별)\n",
        "        if args.target_type == 'grad_block':\n",
        "            proj1_0 = deepcopy(attn_feat[:, 0, :].detach())\n",
        "            proj1_1 = deepcopy(attn_feat[:, 1, :].detach())\n",
        "        elif args.target_type == 'grad_flow':\n",
        "            proj1_0 = attn_feat[:, 0, :]\n",
        "            proj1_1 = attn_feat[:, 1, :]\n",
        "        elif args.target_type == 'project_block':\n",
        "            proj1_0 = projector_0(attn_feat[:, 0, :]).detach()\n",
        "            proj1_1 = projector_1(attn_feat[:, 1, :]).detach()\n",
        "        elif args.target_type == 'project_flow':\n",
        "            proj1_0 = projector_0(attn_feat[:, 0, :])\n",
        "            proj1_1 = projector_1(attn_feat[:, 1, :])\n",
        "\n",
        "        # (4) PatchMix 수행 (mix된 이미지 반환)\n",
        "        mix_attn_feat, origin_feat, label_origin, label_mix, lam, index = model(repeat_mel, y=labels, patch_mix=True, mix_feature=True)\n",
        "\n",
        "        # (5) mix - projector \n",
        "        proj2_0 = projector_0(mix_attn_feat[:, 0, :])  # [B, 128]\n",
        "        proj2_1 = projector_1(mix_attn_feat[:, 1, :])\n",
        "\n",
        "        # (6) GroupMixConLoss & Final loss 계산\n",
        "        loss_con0 = criterion[1](proj1_0, proj2_0, label_origin, label_mix, lam, index)\n",
        "        loss_con1 = criterion[1](proj1_1, proj2_1, label_origin, label_mix, lam, index)\n",
        "        loss = loss_ce + args.alpha * (loss_con0 + loss_con1)\n",
        "        \n",
        "\n",
        "        # (8) Backpopagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # (9) EMA 업데이트\n",
        "        if args.ma_update:\n",
        "            update_ema(model, ema_model, beta=args.ma_beta)\n",
        "            update_ema(projector_0, ema_projector_0, beta=args.ma_beta)\n",
        "            update_ema(projector_1, ema_projector_1, beta=args.ma_beta)\n",
        "            update_ema(classifier, ema_classifier, beta=args.ma_beta)\n",
        "\n",
        "        # (10) Loss 기록\n",
        "        total_train_loss += loss.item()\n",
        "        \n",
        "        # 예측값과 실제값 저장 ( Ablation(4-1) threshold ?? )\n",
        "        predicted = (torch.sigmoid(logits) > 0.5).float()\n",
        "        all_preds.append(predicted.detach().cpu())\n",
        "        all_labels.append(labels.detach().cpu())\n",
        "        all_outputs.append(logits.detach().cpu())\n",
        "\n",
        "\n",
        "    # train loss\n",
        "    train_loss = total_train_loss / len(pretrain_loader)\n",
        "\n",
        "    # Concatenate\n",
        "    all_preds = torch.cat(all_preds, dim=0).numpy()    # shape: [N, 2]\n",
        "    all_labels = torch.cat(all_labels, dim=0).numpy()  # shape: [N, 2]\n",
        "    all_output = torch.cat(all_outputs, dim=0).numpy()\n",
        "\n",
        "    print(f\"[Epoch {epoch} | Train Loss: {train_loss:.4f}, attn_feat: {attn_feat.shape}\")\n",
        "\n",
        "\n",
        "    # =====================================\n",
        "    # 2-Edited. Multi-class 민감도/특이도 계산\n",
        "    # =====================================\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import wandb\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    def multilabel_to_multiclass(y):\n",
        "        # Crackle → 1, Wheeze → 2, Both → 3, None → 0\n",
        "        y = np.array(y)\n",
        "        return y[:, 0] + y[:, 1]*2\n",
        "\n",
        "    def evaluate_multiclass_confusion(y_true, y_pred, class_names=[\"Normal\", \"Wheeze\", \"Crackle\", \"Both\"]):\n",
        "        y_true_cls = multilabel_to_multiclass(y_true)\n",
        "        y_pred_cls = multilabel_to_multiclass(y_pred)\n",
        "\n",
        "        cm = confusion_matrix(y_true_cls, y_pred_cls, labels=[0, 1, 2, 3])\n",
        "\n",
        "        # N_n: 정상 → 정상\n",
        "        N_n = cm[0, 0]\n",
        "        N_total = cm[0].sum()\n",
        "\n",
        "        # 이상 클래스 정답 수: W, C, B\n",
        "        W_total = cm[1].sum()\n",
        "        C_total = cm[2].sum()\n",
        "        B_total = cm[3].sum()\n",
        "\n",
        "        # 각각의 정답 → 정확한 예측만 고려\n",
        "        W_w = cm[1, 1]\n",
        "        C_c = cm[2, 2]\n",
        "        B_b = cm[3, 3]\n",
        "\n",
        "        SP = N_n / (N_total + 1e-6) #spec\n",
        "        SE = (W_w + C_c + B_b) / (W_total + C_total + B_total + 1e-6) #sense\n",
        "\n",
        "        AS = (SP + SE) / 2\n",
        "        HS = 2 * SP * SE / (SP + SE + 1e-6)\n",
        "\n",
        "        return cm, SE, SP, y_true_cls, y_pred_cls\n",
        "\n",
        "    def log_multiclass_conf_matrix_wandb(cm, class_names, sens, spec, normalize, tag):\n",
        "        # Normalize (비율) 옵션\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "            fmt = '.2f'\n",
        "            title = \"Confusion Matrix (Normalized %)\"\n",
        "        else:\n",
        "            fmt = 'd'\n",
        "            title = \"Confusion Matrix (Raw Count)\"\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
        "\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('True')\n",
        "        ax.set_title(title)\n",
        "\n",
        "        icbhi_score = (sens + spec) / 2\n",
        "        # 우하단에 성능 출력\n",
        "        ax.text(\n",
        "            0.99, 0.15,\n",
        "            f\"Sensitivity: {sens*100:.2f}%\\nSpecificity: {spec*100:.2f}%\\nICBHI Score: {icbhi_score*100:.2f}%\",\n",
        "            ha='right', va='bottom',\n",
        "            transform=plt.gca().transAxes,\n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        # wandb.log({tag: wandb.Image(fig)})\n",
        "        # plt.close(fig)\n",
        "        return fig\n",
        "\n",
        "    # 1. 4-class Confusion Matrix 평가\n",
        "    class_names = [\"Normal\", \"Crackle\", \"Wheeze\", \"Both\"]\n",
        "    cm_4x4, train_sens, train_spec, y_true_cls, y_pred_cls = evaluate_multiclass_confusion(all_labels, all_preds, class_names)\n",
        "    icbhi_score = (train_sens + train_spec)/2\n",
        "\n",
        "    print(\"4-Class Confusion Matrix:\\n\", cm_4x4)\n",
        "    print(f\"Sensitivity: {train_sens:.4f}, Specificity: {train_spec:.4f}, ICBHI Score: {icbhi_score:.4f}\")\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 3. Validation\n",
        "    # ===============================\n",
        "    # test_loss, test_labels, test_preds = validate(\n",
        "    #     model, test_loader, criterion, device\n",
        "    # )\n",
        "\n",
        "    test_loss, test_labels, test_preds = validate(\n",
        "        model=ema_model if args.ma_update else model,\n",
        "        classifier=ema_classifier if args.ma_update else classifier,\n",
        "        projector_0=ema_projector_0 if args.ma_update else projector_0,\n",
        "        projector_1=ema_projector_1 if args.ma_update else projector_1,\n",
        "        val_loader=test_loader,\n",
        "        criterion=criterion,\n",
        "        device=device,\n",
        "        args=args\n",
        "    )\n",
        "\n",
        "    precision = precision_score(test_labels, test_preds, average='macro')\n",
        "    recall = recall_score(test_labels, test_preds, average='macro')\n",
        "    f1 = f1_score(test_labels, test_preds, average='macro')\n",
        "\n",
        "    test_cm_4x4, test_sens, test_spec, test_y_true_cls, test_y_pred_cls = evaluate_multiclass_confusion(test_labels, test_preds)\n",
        "    test_icbhi_score = (test_sens+test_spec)/2\n",
        "\n",
        "    print(\"[Validation] Confusion Matrix:\\n\", test_cm_4x4)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"[VALIDATION] Sensitivity: {test_sens:.4f}, Specificity: {test_spec:.4f}, Avg ICBHI Score: {(test_sens+test_spec)/2:.4f}\")\n",
        "    print(\"##################################################\")\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 4. Confusion Matrix\n",
        "    # ===============================\n",
        "\n",
        "    # 2. Finetune Count Confusion Matrix 시각화\n",
        "    fig_finetune_raw = log_multiclass_conf_matrix_wandb(cm_4x4, class_names, train_sens, train_spec, normalize=False, tag=\"Training_conf_matrix_raw\")\n",
        "    fig_finetune_norm = log_multiclass_conf_matrix_wandb(cm_4x4, class_names, train_sens, train_spec, normalize=True, tag=\"Training_conf_matrix_norm\")\n",
        "\n",
        "    # 3. Test Confusion Matrix 시각화\n",
        "    fig_test_raw = log_multiclass_conf_matrix_wandb(test_cm_4x4, class_names, test_sens, test_spec, normalize=False, tag=\"test_conf_matrix_raw\")\n",
        "    fig_test_norm = log_multiclass_conf_matrix_wandb(test_cm_4x4, class_names, test_sens, test_spec, normalize=True, tag=\"test_conf_matrix_norm\")\n",
        "\n",
        "    # 4. log dictionary 생성 -------------------------------------------wan\n",
        "    wandb_log_dict = {\n",
        "        \"finetune_conf_matrix_raw\": wandb.Image(fig_finetune_raw),\n",
        "        \"finetune_conf_matrix_norm\": wandb.Image(fig_finetune_norm),\n",
        "        \"test_conf_matrix_raw\": wandb.Image(fig_test_raw),\n",
        "        \"test_conf_matrix_norm\": wandb.Image(fig_test_norm)\n",
        "    }\n",
        "    # -------------------------------------------wan\n",
        "\n",
        "    # =====================================\n",
        "    # 5. Checkpoint (Every 100 epochs)\n",
        "    # =====================================\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        ckpt_path = CHECKPOINT_PATH + f\"{pretrain_project_name}_{epoch:03d}.pth.tar\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, ckpt_path)\n",
        "        print(f\"💾 Saved checkpoint to {ckpt_path}\")\n",
        "\n",
        "        # =====================================\n",
        "        # EMA 모델 저장 (조건: EMA 활성화일 때)\n",
        "        if args.ma_update:\n",
        "            ema_ckpt_path = CHECKPOINT_PATH + f\"{pretrain_project_name}_ema_{epoch:03d}.pth.tar\"\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': ema_model.state_dict(),\n",
        "                'classifier': ema_classifier.state_dict(),\n",
        "                'projector_0': ema_projector_0.state_dict(),\n",
        "                'projector_1': ema_projector_1.state_dict()\n",
        "            }, ema_ckpt_path)\n",
        "            print(f\"💾 Saved EMA checkpoint to {ema_ckpt_path}\")\n",
        "        # ================================\n",
        "\n",
        "    # ===============================\n",
        "    # 6. Save Best Checkpoint\n",
        "    # ===============================\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        best_epoch = epoch\n",
        "        best_ckpt_path = CHECKPOINT_PATH + f\"{pretrain_project_name}_best.pth.tar\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'loss': best_loss\n",
        "        }, best_ckpt_path)\n",
        "        print(f\"=> Saved best checkpoint (epoch: {epoch}, loss: {best_loss:.4f})\")\n",
        "\n",
        "        # ================================\n",
        "        # EMA 모델 저장\n",
        "        if args.ma_update:\n",
        "            best_ema_ckpt_path = CHECKPOINT_PATH + f\"{pretrain_project_name}_best_ema.pth.tar\"\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': ema_model.state_dict(),\n",
        "                'classifier': ema_classifier.state_dict(),\n",
        "                'projector_0': ema_projector_0.state_dict(),\n",
        "                'projector_1': ema_projector_1.state_dict(),\n",
        "                'loss': best_loss\n",
        "            }, best_ema_ckpt_path)\n",
        "            print(f\"=> Saved best EMA checkpoint (epoch: {epoch}, loss: {best_loss:.4f})\")\n",
        "        # ================================\n",
        "\n",
        "\n",
        "        # 🔹 Confusion Matrix Logging for Best\n",
        "        cm_best, sens_best, spec_best,_, _ = evaluate_multiclass_confusion(test_labels, test_preds, class_names)\n",
        "        fig_best_raw = log_multiclass_conf_matrix_wandb(cm_best, class_names, sens_best, spec_best, normalize=False, tag=\"best_test_conf_matrix_raw\")\n",
        "\n",
        "        fig_best_norm = log_multiclass_conf_matrix_wandb(cm_best, class_names, sens_best, spec_best, normalize=True, tag=\"best_test_conf_matrix_norm\")\n",
        "\n",
        "        # -------------------------------------------wan\n",
        "        wandb_log_dict.update({\n",
        "            \"best_test_conf_matrix_raw\": wandb.Image(fig_best_raw),\n",
        "            \"best_test_conf_matrix_norm\": wandb.Image(fig_best_norm)\n",
        "        })\n",
        "        # -------------------------------------------wan\n",
        "\n",
        "\n",
        "    if epoch == args.epochs - 1:\n",
        "        # 🔸 Confusion Matrix Logging for Last Epoch\n",
        "        cm_last, sens_last, spec_last, _, _  = evaluate_multiclass_confusion(test_labels, test_preds, class_names)\n",
        "        fig_last_raw = log_multiclass_conf_matrix_wandb(cm_last, class_names, sens_last, spec_last, normalize=False, tag=\"last_test_conf_matrix_raw\")\n",
        "\n",
        "        fig_last_norm = log_multiclass_conf_matrix_wandb(cm_last, class_names, sens_last, spec_last, normalize=True, tag=\"last_test_conf_matrix_norm\")\n",
        "\n",
        "        # -------------------------------------------wan\n",
        "        wandb_log_dict.update({\n",
        "            \"last_test_conf_matrix_raw\": wandb.Image(fig_last_raw),\n",
        "            \"last_test_conf_matrix_norm\": wandb.Image(fig_last_norm)\n",
        "        })\n",
        "        # -------------------------------------------wan\n",
        "\n",
        "    # =====================================\n",
        "    # 7. Logging with wandb confusion matrix\n",
        "    # =====================================\n",
        "\n",
        "    # -------------------------------------------wan\n",
        "    # step 1. metrics\n",
        "    wandb.log({\n",
        "        # Train metrics\n",
        "        \"Training/epoch\": epoch,\n",
        "        \"Training/train_loss\": train_loss,\n",
        "        \"Training/test_loss\": test_loss,\n",
        "        \"Training/train_sens\": train_sens,\n",
        "        \"Training/train_spec\": train_spec,\n",
        "        \"Training/icbhi_score\": icbhi_score,\n",
        "\n",
        "        # Test metrics\n",
        "        \"Test/loss\": test_loss,\n",
        "        \"Test/sensitivity\": test_sens,\n",
        "        \"Test/specificity\": test_spec,\n",
        "        \"Test/icbhi_score\": test_icbhi_score\n",
        "    })\n",
        "\n",
        "    # step 2. Confusion matrix\n",
        "    wandb.log(wandb_log_dict)\n",
        "\n",
        "    # -------------------------------------------wan\n",
        "\n",
        "\n",
        "    plt.close(fig_finetune_raw)\n",
        "    plt.close(fig_finetune_norm)\n",
        "    plt.close(fig_test_raw)\n",
        "    plt.close(fig_test_norm)\n",
        "    if 'fig_best_raw' in locals(): plt.close(fig_best_raw)\n",
        "    if 'fig_best_norm' in locals(): plt.close(fig_best_norm)\n",
        "    if 'fig_last_raw' in locals(): plt.close(fig_last_raw)\n",
        "    if 'fig_last_norm' in locals(): plt.close(fig_last_norm)\n",
        "\n",
        "    # # ===============================\n",
        "    # # 8. Scheduler Step\n",
        "    # # ===============================\n",
        "    # scheduler.step()\n",
        "\n",
        "    # ===============================\n",
        "    # 9. Save Metrics\n",
        "    # ===============================\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    train_icbhi_scores.append(icbhi_score)\n",
        "    test_icbhi_scores.append(test_icbhi_score)\n",
        "    epochs.append(epoch)\n",
        "    # ================================\n",
        "\n",
        "    # ================================\n",
        "    # test_labels_all, test_preds_all에 저장\n",
        "    # ================================\n",
        "    test_labels_all.append(test_labels)\n",
        "    test_preds_all.append(test_preds)\n",
        "    # ================================\n",
        "\n",
        "\n",
        "# -------------------------------------------wan\n",
        "wandb.finish()\n",
        "# -------------------------------------------wan\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07e469a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 훈련 종료 후 그래프 ---\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(epochs, train_losses, label='Train Loss')\n",
        "plt.plot(epochs, test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train/Test Loss per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(epochs, train_icbhi_scores, label='Train ICBHI Score')\n",
        "plt.plot(epochs, test_icbhi_scores, label='Test ICBHI Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('ICBHI Score')\n",
        "plt.title('Train/Test ICBHI Score per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "best_epoch_idx = np.argmax(test_icbhi_scores)\n",
        "best_epoch = epochs[best_epoch_idx]\n",
        "best_icbhi_score = test_icbhi_scores[best_epoch_idx]\n",
        "best_test_loss = test_losses[best_epoch_idx]\n",
        "\n",
        "# 최고점 epoch에서의 labels, preds\n",
        "best_test_labels = test_labels_all[best_epoch_idx]\n",
        "best_test_preds = test_preds_all[best_epoch_idx]\n",
        "\n",
        "best_cm, best_sens, best_spec, best_y_true_cls, best_y_pred_cls = evaluate_multiclass_confusion(\n",
        "    best_test_labels, best_test_preds)\n",
        "\n",
        "print(\"\\n=== [최고 Test ICBHI Score 시점 정보] ===\")\n",
        "print(f\"Best Test ICBHI Score: {best_icbhi_score:.4f} (Epoch {best_epoch})\")\n",
        "print(f\"Test Loss at Best: {best_test_loss:.4f}\")\n",
        "print(\"Confusion Matrix at Best ICBHI Score:\")\n",
        "print(best_cm)\n",
        "print(f\"Sensitivity: {best_sens:.4f}, Specificity: {best_spec:.4f}, ICBHI Score: {(best_sens+best_spec)/2:.4f}\")\n",
        "print(f\"Best Epoch: {best_epoch}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "boaz",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
