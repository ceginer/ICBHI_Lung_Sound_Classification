{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ef6b52e2",
      "metadata": {
        "id": "ef6b52e2"
      },
      "source": [
        "#### 환경설정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dc03a42",
      "metadata": {
        "id": "9dc03a42"
      },
      "source": [
        "##### 1. Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04d7d6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f04d7d6f",
        "outputId": "74b32ba8-525f-4d7f-fc2a-5982ec084f6f"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# wandb 로그인\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "992382bf",
      "metadata": {
        "id": "992382bf"
      },
      "source": [
        "##### 2. 라이브러리 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5MISAwpScmYt",
      "metadata": {
        "id": "5MISAwpScmYt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ebed6c5",
      "metadata": {
        "id": "8ebed6c5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torch import Tensor\n",
        "from torchsummary import summary\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d453e5f",
      "metadata": {
        "id": "2d453e5f"
      },
      "source": [
        "##### 3. 경로 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mSXgKx8GoItj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSXgKx8GoItj",
        "outputId": "63d180e3-66a3-49f8-ab7c-d0a594e25b11"
      },
      "outputs": [],
      "source": [
        "ROOT = \"/home/sbw/BOAZ-Chungzins/data/raw\"\n",
        "CHECKPOINT_PATH = \"/home/sbw/boaz/notebook/note_ckp\"\n",
        "PICKLE_PATH = \"/home/sbw/boaz/notebook/pickle\"\n",
        "text = \"/home/sbw/BOAZ-Chungzins/data/metadata/train_test_split.txt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecaaf5a1",
      "metadata": {
        "id": "ecaaf5a1"
      },
      "source": [
        "##### 4. Seed 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f4e372",
      "metadata": {
        "id": "c9f4e372"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
        "\n",
        "seed_everything(42) # Seed 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nydfgBckyPt3",
      "metadata": {
        "id": "nydfgBckyPt3"
      },
      "source": [
        "## 1. Data Load"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce3fdcf1",
      "metadata": {
        "id": "ce3fdcf1"
      },
      "source": [
        "#### 1.1 Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OhJa9jivcg1k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhJa9jivcg1k",
        "outputId": "52b76545-a374-4542-e9e8-596a7d1b5f17"
      },
      "outputs": [],
      "source": [
        "# WAV 파일이 있는 디렉토리 경로\n",
        "data_dir = ROOT\n",
        "txt_dir = ROOT\n",
        "\n",
        "df = pd.read_csv(text, sep='\\t', header=None)\n",
        "\n",
        "# 컬럼 이름 변경\n",
        "df.columns = ['filename', 'set']\n",
        "\n",
        "# train, test split\n",
        "train_df = df[df['set'] == 'train']\n",
        "test_df = df[df['set'] == 'test']\n",
        "\n",
        "# filename list\n",
        "train_list = sorted(train_df['filename'].tolist())\n",
        "test_list = sorted(test_df['filename'].tolist())\n",
        "\n",
        "print(f'Train :{len(train_list)}, Test: {len(test_list)}, Total: {len(train_list) + len(test_list)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04291977",
      "metadata": {
        "id": "04291977"
      },
      "source": [
        "#### 1.2 Pretext-Finetune Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ESBIVnKej0G9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESBIVnKej0G9",
        "outputId": "91f4f11f-f4c6-4145-f8ff-b69818300797"
      },
      "outputs": [],
      "source": [
        "# shuffle train data\n",
        "df_shuffled = train_df.sample(frac=1, random_state=42)\n",
        "\n",
        "# split ratio\n",
        "train_size = int(0.8 * len(df_shuffled))\n",
        "\n",
        "# pretrain, finetune split\n",
        "pretrain_df = df_shuffled[:train_size]\n",
        "finetune_df = df_shuffled[train_size:]\n",
        "\n",
        "# filename list (pretext_list -> pretrain list)\n",
        "pretrain_list = sorted(pretrain_df['filename'].tolist())\n",
        "finetune_list = sorted(finetune_df['filename'].tolist())\n",
        "\n",
        "# patient id list\n",
        "pretrain_patient_list = []\n",
        "for filename in pretrain_list:\n",
        "    number = int(filename.split('_')[0])\n",
        "    pretrain_patient_list.append(number)\n",
        "\n",
        "finetune_patient_list = []\n",
        "for filename in finetune_list:\n",
        "    number = int(filename.split('_')[0])\n",
        "    finetune_patient_list.append(number)\n",
        "\n",
        "pretrain_patient_counts = pd.Series(pretrain_patient_list).value_counts()\n",
        "finetune_patient_counts = pd.Series(finetune_patient_list).value_counts()\n",
        "\n",
        "print(f\"[Pretrain] 환자 수: {len(pretrain_patient_counts.index)}, 샘플 수: {pretrain_patient_counts.sum()}\")\n",
        "print(f\"[Finetune] 환자 수: {len(finetune_patient_counts.index)}, 샘플 수: {finetune_patient_counts.sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oVi6lzuPpSbk",
      "metadata": {
        "id": "oVi6lzuPpSbk"
      },
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8c7719",
      "metadata": {
        "id": "5e8c7719"
      },
      "source": [
        "#### 2.1 Args"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634b232e",
      "metadata": {
        "id": "634b232e"
      },
      "source": [
        "        K: queue size; number of negative keys (default: 65536)\n",
        "        m: moco momentum of updating key encoder (default: 0.999)\n",
        "        T: softmax temperature (default: 0.07)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add5c69b",
      "metadata": {
        "id": "add5c69b"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    # Audio & Spectrogram\n",
        "    target_sr = 4000\n",
        "    frame_size = 256\n",
        "    hop_length = 128    # frame_size 절반\n",
        "    n_mels = 128\n",
        "    target_sec = 8\n",
        "\n",
        "    # Augmentation\n",
        "    time_mask_param = 0.5\n",
        "    freq_mask_param = 0.5\n",
        "\n",
        "    # Train\n",
        "    lr = 0.03\n",
        "    warm = True                     # warm-up 사용 여부\n",
        "    warm_epochs = 10                # warm-up 적용할 초기 epoch 수\n",
        "    warmup_from = lr * 0.1          # warm-up 시작 learning rate (보통 lr의 10%)\n",
        "    warmup_to = lr\n",
        "\n",
        "    batch_size = 128\n",
        "    workers = 4\n",
        "    epochs = 300\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    resume = None\n",
        "    schedule=[120, 160] # schedule\n",
        "\n",
        "    # MLS\n",
        "    K = 1024\n",
        "    momentum = 0.999\n",
        "    T = 0.07\n",
        "    dim_prj = 128\n",
        "    top_k = 15\n",
        "    lambda_bce = 0.5\n",
        "    out_dim = 2048\n",
        "\n",
        "    # Linear Evaluation\n",
        "    ft_epochs = 100\n",
        "\n",
        "    # etc\n",
        "    gpu = 0\n",
        "    data = \"./data_path\"\n",
        "    seed=42\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58e1f949",
      "metadata": {
        "id": "58e1f949"
      },
      "source": [
        "#### 2.2 Utils (func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d2d1329",
      "metadata": {
        "id": "8d2d1329"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# cycle의 클래스를 추출\n",
        "def get_class(cr, wh):\n",
        "    if cr == 1 and wh == 1:\n",
        "        return 3\n",
        "    elif cr == 0 and wh == 1:\n",
        "        return 2\n",
        "    elif cr == 1 and wh == 0:\n",
        "        return 1\n",
        "    elif cr == 0 and wh == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Mel Spectrogram 생성 ( sr=4KHz, frame_size=1024, hop_length=512, n_mels=128 )\n",
        "def generate_mel_spectrogram(waveform, sample_rate, frame_size, hop_length, n_mels):\n",
        "    if hop_length is None:\n",
        "        hop_length = frame_size // 2\n",
        "    mel_spec_transform = T.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_fft=frame_size,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels\n",
        "    )\n",
        "    mel_spectrogram = mel_spec_transform(waveform)\n",
        "    mel_db = T.AmplitudeToDB()(mel_spectrogram)\n",
        "\n",
        "    return mel_db\n",
        "\n",
        "# Cycle Repeat 또는 Crop\n",
        "def repeat_or_truncate_segment(mel_segment, target_frames):\n",
        "    current_frames = mel_segment.shape[-1]\n",
        "    if current_frames >= target_frames:\n",
        "        return mel_segment[:, :, :target_frames]\n",
        "    else:\n",
        "        repeat_ratio = math.ceil(target_frames / current_frames)\n",
        "        mel_segment = mel_segment.repeat(1, 1, repeat_ratio)\n",
        "        return mel_segment[:, :, :target_frames]\n",
        "\n",
        "def preprocess_waveform_segment(waveform, unit_length):\n",
        "\n",
        "    \"\"\"unit_length 기준으로 waveform을 repeat + padding 또는 crop하여 길이 정규화\"\"\"\n",
        "    waveform = waveform.squeeze(0)  # (1, L) → (L,) 로 바꿔도 무방\n",
        "    length_adj = unit_length - len(waveform)\n",
        "\n",
        "    if length_adj > 0:\n",
        "        # waveform이 너무 짧은 경우 → repeat + zero-padding\n",
        "        half_unit = unit_length // 2\n",
        "\n",
        "        if length_adj < half_unit:\n",
        "            # 길이 차이가 작으면 단순 padding\n",
        "            half_adj = length_adj // 2\n",
        "            waveform = F.pad(waveform, (half_adj, length_adj - half_adj))\n",
        "        else:\n",
        "            # 반복 후 부족한 부분 padding\n",
        "            repeat_factor = unit_length // len(waveform)\n",
        "            waveform = waveform.repeat(repeat_factor)[:unit_length]\n",
        "            remaining = unit_length - len(waveform)\n",
        "            half_pad = remaining // 2\n",
        "            waveform = F.pad(waveform, (half_pad, remaining - half_pad))\n",
        "    else:\n",
        "        # waveform이 너무 길면 앞쪽 1/4 내에서 랜덤 crop\n",
        "        length_adj = len(waveform) - unit_length\n",
        "        start = random.randint(0, length_adj // 4)\n",
        "        waveform = waveform[start:start + unit_length]\n",
        "\n",
        "    return waveform.unsqueeze(0)  # 다시 (1, L)로\n",
        "\n",
        "# 데이터 Spec Augmentation ( 0~80% Random Masking )\n",
        "def apply_spec_augment(mel_segment):\n",
        "\n",
        "    M = mel_segment.shape[-1]\n",
        "    F = mel_segment.shape[-2]\n",
        "\n",
        "    # torchaudio의 마스킹은 0부터 mask_param까지 균등분포에서 랜덤하게 길이를 선택\n",
        "    time_masking = T.TimeMasking(time_mask_param=int(M * 0.8))\n",
        "    freq_masking = T.FrequencyMasking(freq_mask_param=int(F * 0.8) )\n",
        "\n",
        "    aug1 = freq_masking(mel_segment.clone())\n",
        "    aug2 = time_masking(mel_segment.clone())\n",
        "    aug3 = freq_masking(time_masking(mel_segment.clone()))\n",
        "\n",
        "    return aug1, aug2, aug3\n",
        "\n",
        "# Waveform resample\n",
        "def resample_waveform(waveform, orig_sr, target_sr=args.target_sr):\n",
        "    if orig_sr != target_sr:\n",
        "        resampler = torchaudio.transforms.Resample(\n",
        "            orig_freq=orig_sr,\n",
        "            new_freq=target_sr\n",
        "        )\n",
        "        return resampler(waveform), target_sr\n",
        "    return waveform, orig_sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70af5e6b",
      "metadata": {
        "id": "70af5e6b"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# -------------------- Augmentation functions (ICBHI 멜스펙트로그램에 최적화) --------------------\n",
        "\n",
        "def spec_augment(mel, time_mask_ratio=0.15, freq_mask_ratio=0.15):\n",
        "    \"\"\"\n",
        "    SpecAugment: 시간/주파수 영역 마스킹\n",
        "    - 시간축 마스킹: 63 * 0.15 ≈ 9 프레임\n",
        "    - 주파수 마스킹: 128 * 0.1 ≈ 12 채널\n",
        "    \"\"\"\n",
        "    M = mel.shape[-1]  # 시간 축\n",
        "    F = mel.shape[-2]  # 주파수 축\n",
        "\n",
        "    time_masking = T.TimeMasking(time_mask_param=max(1, int(M * time_mask_ratio)))\n",
        "    freq_masking = T.FrequencyMasking(freq_mask_param=max(1, int(F * freq_mask_ratio)))\n",
        "\n",
        "    mel = freq_masking(mel.clone())\n",
        "    mel = time_masking(mel)\n",
        "    return mel\n",
        "\n",
        "def add_noise(mel, noise_level=0.001):\n",
        "    \"\"\"\n",
        "    노이즈 추가: 적당한 수준의 표준 정규분포 노이즈 (너무 높으면 손실 커짐)\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(mel) * noise_level\n",
        "    return mel + noise\n",
        "\n",
        "def pitch_shift(mel, n_steps=2):\n",
        "    \"\"\"\n",
        "    주파수 축 순환 이동 (mel axis). shape은 그대로 유지됨.\n",
        "    n_steps=2면 ±2 멜 채널만 이동.\n",
        "    \"\"\"\n",
        "    shift = random.randint(-n_steps, n_steps)\n",
        "    if shift == 0:\n",
        "        return mel\n",
        "    if shift > 0:\n",
        "        mel = torch.cat([mel[:, :, shift:, :], mel[:, :, :shift, :]], dim=2)\n",
        "    else:\n",
        "        shift = abs(shift)\n",
        "        mel = torch.cat([mel[:, :, -shift:, :], mel[:, :, :-shift, :]], dim=2)\n",
        "    return mel\n",
        "\n",
        "def time_stretch(mel, min_rate=0.95, max_rate=1.05):\n",
        "    \"\"\"\n",
        "    시간 축 길이 조절. 너무 심하지 않게 ±5% 범위로만 조정.\n",
        "    - shape 유지 위해 interpolation 후 crop/pad\n",
        "    \"\"\"\n",
        "    rate = random.uniform(min_rate, max_rate)\n",
        "    if rate == 1.0:\n",
        "        return mel\n",
        "\n",
        "    orig_size = mel.shape[-1]\n",
        "    target_size = int(orig_size * rate)\n",
        "\n",
        "    mel_stretched = F.interpolate(\n",
        "        mel, size=(mel.shape[-2], target_size),  # (mel_bins, time)\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "    if target_size > orig_size:\n",
        "        return mel_stretched[..., :orig_size]\n",
        "    else:\n",
        "        pad = orig_size - target_size\n",
        "        return F.pad(mel_stretched, (0, pad))\n",
        "\n",
        "# -------------------- Dispatcher --------------------\n",
        "\n",
        "AUGMENTATION_FUNCTIONS_TORCH = {\n",
        "    \"spec_augment\": spec_augment,\n",
        "    \"add_noise\": add_noise,\n",
        "    \"pitch_shift\": pitch_shift,\n",
        "    \"time_stretch\": time_stretch\n",
        "}\n",
        "\n",
        "def apply_augmentations_torch(x, methods=[], **kwargs):\n",
        "    for method in methods:\n",
        "        func = AUGMENTATION_FUNCTIONS_TORCH.get(method)\n",
        "        if func is None:\n",
        "            raise ValueError(f\"Unknown augmentation: {method}\")\n",
        "        x = func(x, **kwargs.get(method, {}))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a62aa74",
      "metadata": {
        "id": "4a62aa74"
      },
      "outputs": [],
      "source": [
        "def aug(repeat_mel):\n",
        "    # 먼저 복사본 준비\n",
        "    mel1 = repeat_mel.clone()\n",
        "    mel2 = repeat_mel.clone()\n",
        "\n",
        "    # 각각 다른 증강 A, B 적용\n",
        "    aug1 = apply_augmentations_torch(mel1, methods=[\"add_noise\"], add_noise={\"noise_level\": 0.005})\n",
        "    aug2 = apply_augmentations_torch(mel2, methods=[\"time_stretch\"], time_stretch={\"min_rate\": 0.8, \"max_rate\": 1.2})\n",
        "    # aug3 = apply_augmentations_torch(mel3, methods=[\"pitch_shift\"], pitch_shift={\"n_steps\": 2})\n",
        "\n",
        "    # # 각 결과에 spec_augment 추가 적용\n",
        "    aug1_spec = spec_augment(aug1, time_mask_ratio=0.6, freq_mask_ratio=0.4)\n",
        "    aug2_spec = spec_augment(aug2, time_mask_ratio=0.6, freq_mask_ratio=0.4)\n",
        "    # aug3_spec = spec_augment(aug3, time_mask_ratio=0.6, freq_mask_ratio=0.4)\n",
        "\n",
        "    return aug1_spec, aug2_spec, None\n",
        "\n",
        "\n",
        "def get_timestamp():\n",
        "    \"\"\"Outputs current time in KST like 2404070830\"\"\"\n",
        "    kst_time = datetime.now(ZoneInfo(\"Asia/Seoul\"))\n",
        "    return kst_time.strftime('%y%m%d%H%M')\n",
        "\n",
        "# Origin\n",
        "# def aug(repeat_mel):\n",
        "#     aug1, aug2, aug3 = apply_spec_augment(repeat_mel)\n",
        "#     return aug1, aug2, aug3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e684cb",
      "metadata": {
        "id": "39e684cb"
      },
      "source": [
        "#### 2.3 CycleDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1642a79a",
      "metadata": {
        "id": "1642a79a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CycleDataset(Dataset):\n",
        "    def __init__(self, filename_list, wav_dir, txt_dir, target_sec=args.target_sec, target_sr=args.target_sr, frame_size=args.frame_size, hop_length=args.hop_length, n_mels=args.n_mels):\n",
        "        self.filename_list = filename_list\n",
        "        self.wav_dir = wav_dir\n",
        "        self.txt_dir = txt_dir\n",
        "        self.target_sec = target_sec\n",
        "        self.target_sr = target_sr\n",
        "        self.frame_size = frame_size\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mels = n_mels\n",
        "\n",
        "        self.cycle_list = []\n",
        "\n",
        "        print(\"[INFO] Preprocessing cycles...\")\n",
        "        for filename in tqdm(self.filename_list):\n",
        "            txt_path = os.path.join(self.txt_dir, filename + '.txt')\n",
        "            wav_path = os.path.join(self.wav_dir, filename + '.wav')\n",
        "\n",
        "            if not os.path.exists(txt_path):\n",
        "                print(f\"[WARNING] Missing file: {txt_path}\")\n",
        "            if not os.path.exists(wav_path):\n",
        "                print(f\"[WARNING] Missing file: {wav_path}\")\n",
        "\n",
        "            # Load annotation\n",
        "            cycle_data = np.loadtxt(txt_path, usecols=(0, 1))\n",
        "            lung_label = np.loadtxt(txt_path, usecols=(2, 3))\n",
        "\n",
        "            # Load waveform\n",
        "            waveform, orig_sr = torchaudio.load(wav_path)\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)  # Stereo to mono\n",
        "\n",
        "            # Resample to target sample rate (4kHz)\n",
        "            waveform, sample_rate = resample_waveform(waveform, orig_sr, self.target_sr)\n",
        "\n",
        "            for idx in range(len(cycle_data)):\n",
        "                # 호흡 주기 start, end\n",
        "                start_sample = int(cycle_data[idx, 0] * sample_rate)\n",
        "                end_sample = int(cycle_data[idx, 1] * sample_rate)\n",
        "                lung_duration = cycle_data[idx, 1] - cycle_data[idx, 0]\n",
        "\n",
        "                if end_sample <= start_sample:\n",
        "                    continue  # 잘못된 구간 스킵\n",
        "\n",
        "                # Waveform repeat + padding 후 Mel_db\n",
        "                cycle_wave = waveform[:, start_sample:end_sample]\n",
        "                normed_wave = preprocess_waveform_segment(cycle_wave, unit_length=int(self.target_sec * self.target_sr))\n",
        "                mel = generate_mel_spectrogram(normed_wave, sample_rate, frame_size=self.frame_size, hop_length=self.hop_length, n_mels=self.n_mels)\n",
        "\n",
        "                # crackle, wheeze -> class\n",
        "                cr = int(lung_label[idx, 0])\n",
        "                wh = int(lung_label[idx, 1])\n",
        "                label = get_class(cr, wh)\n",
        "\n",
        "                multi_label = torch.tensor([\n",
        "                    float(label in [1, 3]),\n",
        "                    float(label in [2, 3])\n",
        "                ])  # 변환된 multi-label 반환\n",
        "\n",
        "                # meta_data\n",
        "                meta_data = (filename, lung_duration)\n",
        "\n",
        "                self.cycle_list.append((mel, multi_label, meta_data))\n",
        "\n",
        "        print(f\"[INFO] Total cycles collected: {len(self.cycle_list)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cycle_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mel, label, meta_data = self.cycle_list[idx]\n",
        "        return mel, label, meta_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55d5a070",
      "metadata": {
        "id": "55d5a070"
      },
      "source": [
        "##### Pickle.dump"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752088df",
      "metadata": {
        "id": "752088df"
      },
      "source": [
        "CycleDataset 객체 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d386e82",
      "metadata": {
        "id": "9d386e82"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "wav_dir = ROOT\n",
        "txt_dir = ROOT\n",
        "\n",
        "# 1. Dataset 로드\n",
        "train_dataset = CycleDataset(train_list, wav_dir, txt_dir)\n",
        "test_dataset = CycleDataset(test_list, wav_dir, txt_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4BQgVyGnrDbN",
      "metadata": {
        "id": "4BQgVyGnrDbN"
      },
      "source": [
        "pickle로 train_dataset, test_dataset 외부 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a273735",
      "metadata": {
        "id": "7a273735"
      },
      "outputs": [],
      "source": [
        "pickle_name = f'MLS_{args.target_sr//1000}kHz_{args.frame_size}win_{args.hop_length}hop_{args.n_mels}mel_{args.target_sec}s'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd34caa0",
      "metadata": {
        "id": "cd34caa0"
      },
      "outputs": [],
      "source": [
        "pickle_dict = {\n",
        "    'train_dataset': train_dataset,\n",
        "    'test_dataset': test_dataset\n",
        "}\n",
        "\n",
        "save_path = os.path.join(PICKLE_PATH, pickle_name + '.pkl')\n",
        "with open(save_path, 'wb') as f:\n",
        "    pickle.dump(pickle_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zRqSwthYTtxq",
      "metadata": {
        "id": "zRqSwthYTtxq"
      },
      "outputs": [],
      "source": [
        "# # 2. 간단 통계\n",
        "# print(f\"Total cycles: {len(train_dataset)}\")\n",
        "\n",
        "# label_counter = [0] * 4  # normal, crackle, wheeze, both\n",
        "# for _, multi_label,_ in train_dataset:\n",
        "#     if torch.equal(multi_label, torch.tensor([0., 0.])):\n",
        "#         label_counter[0] += 1\n",
        "#     elif torch.equal(multi_label, torch.tensor([1., 0.])):\n",
        "#         label_counter[1] += 1\n",
        "#     elif torch.equal(multi_label, torch.tensor([0., 1.])):\n",
        "#         label_counter[2] += 1\n",
        "#     elif torch.equal(multi_label, torch.tensor([1., 1.])):\n",
        "#         label_counter[3] += 1\n",
        "\n",
        "# for idx, count in enumerate(label_counter):\n",
        "#     print(f\"Class {idx}: {count} cycles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yJPtbC3BrqAE",
      "metadata": {
        "id": "yJPtbC3BrqAE"
      },
      "source": [
        "##### Pickle.load\n",
        "저장된 train_dataset, test_dataset을 로드  \n",
        "(> Aug 는 Moco 모델에서 사용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabc0ae8",
      "metadata": {},
      "outputs": [],
      "source": [
        "pickle_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EWrjdCFSrmER",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWrjdCFSrmER",
        "outputId": "6b437dce-f371-4bbc-c198-643ab1dc1c5e"
      },
      "outputs": [],
      "source": [
        "save_path = os.path.join(PICKLE_PATH, pickle_name + '.pkl')\n",
        "with open(save_path, 'rb') as f:\n",
        "    pickle_dict = pickle.load(f)\n",
        "\n",
        "train_dataset = pickle_dict['train_dataset']\n",
        "test_dataset = pickle_dict['test_dataset']\n",
        "\n",
        "print(f\"[Train] Cycles: {len(train_dataset)}\")\n",
        "print(f\"[Test] Cycles: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceff18b5",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bcca3481",
      "metadata": {
        "id": "bcca3481"
      },
      "source": [
        "#### 2.4 DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f19b4a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f19b4a7",
        "outputId": "933e3a6c-0f75-4610-b9dc-385eaf937da8"
      },
      "outputs": [],
      "source": [
        "# ---------------- 학습 데이터 구성(seed) ----------------\n",
        "seed_everything(args.seed)\n",
        "\n",
        "# train_dataset 내에서 각 파일의 인덱스를 추출\n",
        "pretrain_idx = []\n",
        "finetune_idx = []\n",
        "\n",
        "for i in range(len(train_dataset)):\n",
        "    filename = train_dataset[i][2][0]\n",
        "\n",
        "    if filename in pretrain_list:\n",
        "        pretrain_idx.append(i)\n",
        "    elif filename in finetune_list:\n",
        "        finetune_idx.append(i)\n",
        "\n",
        "# 인덱스 순서 셔플\n",
        "random.shuffle(pretrain_idx)\n",
        "random.shuffle(finetune_idx)\n",
        "\n",
        "print(f\"Pretrain set size: {len(pretrain_idx)}, Finetune set size: {len(finetune_idx)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce2c3c8",
      "metadata": {
        "id": "cce2c3c8"
      },
      "source": [
        "코드 실행 환경에 따라 num_workers를 적절한 값으로 지정해주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432ae0cd",
      "metadata": {
        "id": "432ae0cd"
      },
      "outputs": [],
      "source": [
        "# Dataset 생성 (Subset)\n",
        "pretrain_dataset = Subset(train_dataset, pretrain_idx)\n",
        "finetune_dataset = Subset(train_dataset, finetune_idx)\n",
        "\n",
        "# DataLoader 생성\n",
        "pretrain_loader = DataLoader(\n",
        "    pretrain_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=0,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "finetune_loader = DataLoader(\n",
        "    finetune_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=0,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8iNQDqY0Su9h",
      "metadata": {
        "id": "8iNQDqY0Su9h"
      },
      "outputs": [],
      "source": [
        "seed_everything(42)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    num_workers=4,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b492ad67",
      "metadata": {
        "id": "b492ad67"
      },
      "source": [
        "label 분포 확인 (단순 참고용, 실제 환경에서는 pretrain set의 label 분포가 어떤지 알 수 없음)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea9d290",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fea9d290",
        "outputId": "7c7940fe-2d7c-4085-9cb9-3a43d1ec925e"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# label\n",
        "labels = torch.stack([multi_label for _, multi_label, _ in train_dataset])\n",
        "\n",
        "# pretext와 finetune 데이터셋의 라벨 분포 출력\n",
        "pretrain_labels = labels[pretrain_idx]\n",
        "pretrain_labels_class = (\n",
        "    pretrain_labels[:, 0].long() * 1 +  # crackle bit → *1\n",
        "    pretrain_labels[:, 1].long() * 2    # wheeze bit  → *2\n",
        ")  # [N] shape, values in {0, 1, 2, 3}\n",
        "finetune_labels = labels[finetune_idx]\n",
        "finetune_labels_class = (\n",
        "    finetune_labels[:, 0].long() * 1 +  # crackle bit → *1\n",
        "    finetune_labels[:, 1].long() * 2    # wheeze bit  → *2\n",
        ")  # [N] shape, values in {0, 1, 2, 3}\n",
        "\n",
        "# test 데이터셋의 라벨 분포 출력\n",
        "test_labels = torch.stack([multi_label for _, multi_label, _ in test_dataset])\n",
        "test_labels_class = (\n",
        "    test_labels[:, 0].long() * 1 +  # crackle bit → *1\n",
        "    test_labels[:, 1].long() * 2    # wheeze bit  → *2\n",
        ")  # [N] shape, values in {0, 1, 2, 3}\n",
        "\n",
        "print(f\"Pretrain sample: {len(pretrain_labels_class)}\")\n",
        "print(\"Pretrain label distribution:\", Counter(pretrain_labels_class.tolist()))\n",
        "print(f\"\\nFinetune sample: {len(finetune_labels_class)}\")\n",
        "print(\"Finetune label distribution:\", Counter(finetune_labels_class.tolist()))\n",
        "print(f\"\\nTest sample: {len(test_labels_class)}\")\n",
        "print(\"Test label distribution:\", Counter(test_labels_class.tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed559ff1",
      "metadata": {
        "id": "ed559ff1"
      },
      "source": [
        "## 3. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca710799",
      "metadata": {
        "id": "ca710799"
      },
      "source": [
        "#### 3.1 Pre-trained ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2caf4a6c",
      "metadata": {
        "id": "2caf4a6c"
      },
      "outputs": [],
      "source": [
        "def backbone_resnet():\n",
        "    # 1. 기본 ResNet50 생성 (pretrained=False로 시작)\n",
        "    resnet = models.resnet50(pretrained=False)\n",
        "\n",
        "    # 2. 첫 번째 conv 레이어를 1채널용으로 수정\n",
        "    resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "    # 먼저 fc 제거\n",
        "    resnet.fc = nn.Identity()\n",
        "\n",
        "    # 3. ImageNet 가중치 로드 (conv1 제외)\n",
        "    state_dict = load_state_dict_from_url(\n",
        "        'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "        progress=True\n",
        "    )\n",
        "    if 'conv1.weight' in state_dict:\n",
        "        del state_dict['conv1.weight']\n",
        "    resnet.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    return resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K7EFbqsBSk9E",
      "metadata": {
        "id": "K7EFbqsBSk9E"
      },
      "source": [
        "ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KQtvFgzaSjTO",
      "metadata": {
        "id": "KQtvFgzaSjTO"
      },
      "outputs": [],
      "source": [
        "# from torchvision import models\n",
        "# from torch.hub import load_state_dict_from_url\n",
        "# import torch.nn as nn\n",
        "\n",
        "# def backbone_resnet():\n",
        "#     # 1. 기본 ResNet18 생성\n",
        "#     resnet = models.resnet18(pretrained=False)\n",
        "\n",
        "#     # 2. 첫 번째 conv 레이어를 1채널용으로 수정\n",
        "#     resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "#     # fc 제거\n",
        "#     resnet.fc = nn.Identity()\n",
        "\n",
        "#     # 3. ImageNet 가중치 로드 (conv1 제외)\n",
        "#     state_dict = load_state_dict_from_url(\n",
        "#         'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n",
        "#         progress=True\n",
        "#     )\n",
        "#     if 'conv1.weight' in state_dict:\n",
        "#         del state_dict['conv1.weight']\n",
        "#     resnet.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "#     return resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09f21daa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09f21daa",
        "outputId": "08fc745f-c109-4d85-cbe4-4e55638aca18"
      },
      "outputs": [],
      "source": [
        "# summary 함수 사용: (채널, 높이, 너비) 크기를 지정\n",
        "summary(backbone_resnet().to(device), input_size=(1, 224, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0305c74",
      "metadata": {
        "id": "e0305c74"
      },
      "source": [
        "#### 3.2 MoCo (MLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ska5GunlcKzI",
      "metadata": {
        "id": "ska5GunlcKzI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# K: queue_g의 크기\n",
        "# dim_enc: projector 통과 전, g1,g2 벡터의 차원\n",
        "# dim_prj: projector 통과 후, z1,z2 벡터의 차원\n",
        "class MoCo(nn.Module):\n",
        "    def __init__(self, base_encoder, dim_enc=args.out_dim, dim_prj=128, K=512, m=0.999, T=0.07, top_k=10, lambda_bce=0.5):\n",
        "        super().__init__()\n",
        "        self.K = K\n",
        "        self.m = m\n",
        "        self.T = T\n",
        "        self.top_k = top_k\n",
        "        self.lambda_bce = lambda_bce\n",
        "\n",
        "        self.encoder_q = base_encoder()\n",
        "        self.encoder_k = base_encoder()\n",
        "\n",
        "        dim_enc = dim_enc\n",
        "        self.proj_head_q = nn.Sequential(\n",
        "            nn.Linear(dim_enc, dim_enc),\n",
        "            nn.BatchNorm1d(dim_enc),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim_enc, dim_prj)\n",
        "        )\n",
        "        self.proj_head_k = nn.Sequential(\n",
        "            nn.Linear(dim_enc, dim_enc),\n",
        "            nn.BatchNorm1d(dim_enc),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim_enc, dim_prj)\n",
        "        )\n",
        "\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)\n",
        "            param_k.requires_grad = False\n",
        "\n",
        "        self.register_buffer(\"queue_g\", F.normalize(torch.randn(dim_enc, K), dim=0))      # g2를 정규화한 후 열 단위로 Qg에 저장\n",
        "        self.register_buffer(\"queue_z\", F.normalize(torch.randn(dim_prj, K), dim=0))      # z2를 정규화한 후 열 단위로 Qz에 저장\n",
        "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))               # 현재 queue에 새로 쓸 위치(인덱스)를 추적하는 포인터 역할\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update_key_encoder(self):\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _dequeue_and_enqueue(self, g2, z2):\n",
        "        batch_size = g2.shape[0]\n",
        "        ptr = int(self.queue_ptr)\n",
        "        assert self.K % batch_size == 0\n",
        "        self.queue_g[:, ptr:ptr+batch_size] = g2.T.detach()\n",
        "        self.queue_z[:, ptr:ptr+batch_size] = z2.T.detach()\n",
        "        self.queue_ptr[0] = (ptr + batch_size) % self.K\n",
        "\n",
        "    def forward(self, im_q, im_k, epoch=None, warmup_epochs=10):\n",
        "        # encoder_q → g1 (feature)\n",
        "        g1 = F.normalize(self.encoder_q(im_q), dim=1)  # shape: [B, 2048]\n",
        "\n",
        "        # projection head → z1\n",
        "        z1 = F.normalize(self.proj_head_q(g1), dim=1)  # shape: [B, 128]\n",
        "\n",
        "        # encoder k\n",
        "        with torch.no_grad():\n",
        "            self._momentum_update_key_encoder()\n",
        "            g2 = F.normalize(self.encoder_k(im_k), dim=1)\n",
        "            z2 = F.normalize(self.proj_head_k(g2), dim=1)\n",
        "\n",
        "        # top-k mining\n",
        "        sim_g = torch.matmul(g1, self.queue_g.clone().detach())  # [N, K]\n",
        "        # Ablation(1-1) Hard top-k\n",
        "        topk_idx = torch.topk(sim_g, self.top_k, dim=1).indices\n",
        "        y = torch.zeros_like(sim_g)\n",
        "        y.scatter_(1, topk_idx, 1.0)\n",
        "        # # Ablation(1-2) Soft top-k\n",
        "        # topk_sim, topk_idx = torch.topk(sim_g, self.top_k, dim=1)\n",
        "        # y = torch.zeros_like(sim_g)\n",
        "        # y.scatter_(1, topk_idx, F.softmax(topk_sim / self.T, dim=1))\n",
        "\n",
        "        ##################################################################\n",
        "        # logits from z1 · Qz\n",
        "        sim_z = torch.matmul(z1, self.queue_z.clone().detach())\n",
        "        # Ablation(2-1) BCE Loss\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(sim_z / self.T, y) # 개선-> sigmoid(sim_z), 1/D\n",
        "\n",
        "        # # Ablation(2-2) Weighted BCE Loss\n",
        "        # pos_weight = torch.ones_like(sim_z) * (self.K / self.top_k)\n",
        "        # bce_loss = F.binary_cross_entropy_with_logits(sim_z / self.T, y, pos_weight=pos_weight)\n",
        "\n",
        "        ###################################################################\n",
        "        # InfoNCE loss\n",
        "        l_pos = torch.sum(z1 * z2, dim=1, keepdim=True)\n",
        "        l_neg = torch.matmul(z1, self.queue_z.clone().detach())\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1) / self.T\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(logits.device)\n",
        "        info_nce_loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Total loss (with optional warmup) # MLS 논문에서는 warmup 아예 안쓴다고 함\n",
        "        if epoch is not None and epoch < warmup_epochs:\n",
        "            loss = info_nce_loss\n",
        "        # else:\n",
        "        loss = info_nce_loss + self.lambda_bce * bce_loss\n",
        "        # print(f\"INFO_NCE: {info_nce_loss}\")\n",
        "        # print(f\"TRIPLET: {triplet_loss}\")\n",
        "        # print(f\"BCE: {bce_loss}\")\n",
        "\n",
        "        self._dequeue_and_enqueue(g2, z2)\n",
        "\n",
        "        return loss, logits, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc1bf21",
      "metadata": {
        "id": "1cc1bf21"
      },
      "source": [
        "## 4. Pretrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-BkAfkqhyHrY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BkAfkqhyHrY",
        "outputId": "e44fb42a-b0d8-4542-a7c3-5d458437010b"
      },
      "outputs": [],
      "source": [
        "next(iter(pretrain_loader))[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iWjL-9oSm76P",
      "metadata": {
        "id": "iWjL-9oSm76P"
      },
      "outputs": [],
      "source": [
        "pretrain_project_name = f'SBW_aug(T.N)_PT_{args.target_sr}sr_{args.n_mels}mels_{args.batch_size}bs_top{args.top_k}_{args.lambda_bce}ld_{get_timestamp()}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e745fdd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "e745fdd5",
        "outputId": "d23d5744-7e9a-4e82-dcb1-222e8e9e89b8"
      },
      "outputs": [],
      "source": [
        "# 모델 지정하기 전 seed 고정 필요\n",
        "seed_everything(args.seed) # Seed 고정\n",
        "\n",
        "wandb.init(\n",
        "    project=\"SBW_ICBHI_MLS\",           # 프로젝트 이름\n",
        "    name=f\"{pretrain_project_name}\", # 실험 이름\n",
        "    config={\n",
        "        \"epochs\": args.ft_epochs,\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"lr\": args.lr,\n",
        "        \"momentum\": args.momentum,\n",
        "        \"weight_decay\": args.weight_decay,\n",
        "    }\n",
        ")\n",
        "\n",
        "# 1. MoCo 모델 생성\n",
        "model = MoCo(\n",
        "    base_encoder = backbone_resnet,\n",
        "    dim_enc = args.out_dim,\n",
        "    dim_prj = args.dim_prj,\n",
        "    K = args.K,\n",
        "    m = args.momentum,\n",
        "    T = args.T,\n",
        "    top_k = args.top_k,\n",
        "    lambda_bce = args.lambda_bce\n",
        ").cuda()\n",
        "\n",
        "# 2. Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), args.lr, weight_decay=args.weight_decay)\n",
        "# optimizer = torch.optim.SGD(\n",
        "#     model.parameters(),\n",
        "#     lr=args.lr,\n",
        "#     momentum=0.9,\n",
        "#     weight_decay=args.weight_decay,\n",
        "#     nesterov=True\n",
        "# )\n",
        "\n",
        "# 3. Cosine Scheduler\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=1e-6)\n",
        "\n",
        "# 4. Train\n",
        "# Best loss 초기화\n",
        "best_loss = float('inf')\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    # ===============================\n",
        "    # Training\n",
        "    # ===============================\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for i, (repeat_mel, label, _) in enumerate(pretrain_loader): # label 여기선 사용 X\n",
        "        im_q, im_k, _ = aug(repeat_mel)\n",
        "\n",
        "        # scaling augs\n",
        "        im_q = (im_q - im_q.mean() ) / (im_q.std() + 1e-6)\n",
        "        im_k = (im_k - im_k.mean() ) / (im_k.std() + 1e-6)\n",
        "\n",
        "        im_q = im_q.cuda(device=args.gpu, non_blocking=True)\n",
        "        im_k = im_k.cuda(device=args.gpu, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, output, target = model(im_q=im_q, im_k=im_k)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(pretrain_loader)\n",
        "    print(f\"Epoch {epoch} | Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"[Epoch {epoch} | Step {i}] im_q: {im_q.shape}, im_k: {im_k.shape}\")\n",
        "\n",
        "    # =====================================\n",
        "    # Scheduler\n",
        "    # =====================================\n",
        "    scheduler.step()\n",
        "\n",
        "    # # =====================================\n",
        "    # Logging with wandb\n",
        "    # =====================================\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    wandb.log({\n",
        "        # \"epoch\": epoch,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        # \"lr\": current_lr\n",
        "    })\n",
        "\n",
        "    # =====================================\n",
        "    # Checkpoint (Every 100 epochs)\n",
        "    # =====================================\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        ckpt_path = CHECKPOINT_PATH + f\"/{pretrain_project_name}_{epoch:03d}.pth.tar\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, ckpt_path)\n",
        "        print(f\"💾 Saved checkpoint to {ckpt_path}\")\n",
        "\n",
        "    # ===============================\n",
        "    # Save Best Checkpoint\n",
        "    # ===============================\n",
        "    if avg_train_loss < best_loss:\n",
        "        best_loss = avg_train_loss\n",
        "        best_epoch = epoch\n",
        "        best_ckpt_path = CHECKPOINT_PATH + f\"/{pretrain_project_name}_best_checkpoint.pth.tar\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'loss': best_loss\n",
        "        }, best_ckpt_path)\n",
        "        print(f\"=> Saved best checkpoint (epoch: {epoch}, loss: {best_loss:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sZSwN7t7l2n5",
      "metadata": {
        "id": "sZSwN7t7l2n5"
      },
      "source": [
        "## 5. Linear Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "646538df",
      "metadata": {
        "id": "646538df"
      },
      "source": [
        "#### validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33da3ad9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33da3ad9",
        "outputId": "006848a5-df01-46c1-a32b-d056cfba130b"
      },
      "outputs": [],
      "source": [
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e66818a3",
      "metadata": {
        "id": "e66818a3"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, _ in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).int()  # threshold = 0.5\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0).numpy()   # [N, 2]\n",
        "    all_labels = torch.cat(all_labels, dim=0).numpy() # [N, 2]\n",
        "\n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "    return avg_loss, all_labels, all_preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3b5fa31",
      "metadata": {
        "id": "d3b5fa31"
      },
      "source": [
        "### Weighted loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4475c581",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4475c581",
        "outputId": "85beccdf-e6f9-4b87-fbd9-5e6f76cfbd86"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 💡 다중 라벨 예시: targets는 [B, C] binary matrix (e.g., [1, 0, 1, 0])\n",
        "label_list = []\n",
        "\n",
        "# 👇 train_dataset이 (x, multi_label_tensor, _) 형태라고 가정\n",
        "for _, label, _ in test_dataset:\n",
        "    label_list.append(label)  # label: Tensor([0, 1, 0, 1])처럼\n",
        "\n",
        "# 전체 label을 합치기\n",
        "all_labels = torch.stack(label_list, dim=0)  # shape: [N, C]\n",
        "num_classes = all_labels.size(1)\n",
        "total_samples = all_labels.size(0)\n",
        "\n",
        "# 클래스별 1의 개수 세기\n",
        "class_counts = all_labels.sum(dim=0)  # shape: [C]\n",
        "class_weights = total_samples / (num_classes * class_counts + 1e-6)  # smoothed\n",
        "\n",
        "# tensor로 변환\n",
        "class_weights_tensor = class_weights.float().to(device)\n",
        "\n",
        "# 🔹 출력\n",
        "for i, count in enumerate(class_counts.tolist()):\n",
        "    print(f\"Class {i} - Positives (1): {int(count)} / {total_samples} samples\")\n",
        "print(f\"Class Weights: {class_weights_tensor}\")\n",
        "\n",
        "alpha_norm = class_weights_tensor / class_weights_tensor.sum()\n",
        "print(f\"alpha_norm: {alpha_norm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N6RiJJnHjnRg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6RiJJnHjnRg",
        "outputId": "fcd859eb-70ac-45dd-a04d-72738db6aafe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# ⚙️ 각 클래스의 positive 개수 (from label distribution)\n",
        "crackle_pos = 262 + 83  # label 1 or 3\n",
        "wheeze_pos  = 84 + 83   # label 2 or 3\n",
        "\n",
        "total_samples = 885\n",
        "num_classes = 2\n",
        "\n",
        "# ⚖️ 기본 class weight 계산: inverse frequency\n",
        "class_counts = torch.tensor([crackle_pos, wheeze_pos], dtype=torch.float)\n",
        "class_weights = total_samples / (num_classes * class_counts + 1e-6)\n",
        "\n",
        "# ✅ 정규화: sum = 1\n",
        "alpha_norm = class_weights / class_weights.sum()\n",
        "\n",
        "# 출력\n",
        "print(\"Raw Class Weights:\", class_weights)\n",
        "print(\"Normalized Alpha (sum=1):\", alpha_norm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e58ee2",
      "metadata": {
        "id": "86e58ee2"
      },
      "source": [
        "### Multi-label Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d6150e",
      "metadata": {
        "id": "72d6150e"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiLabelFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # Tensor of shape [C], or scalar\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        logits: [B, C] - raw scores\n",
        "        targets: [B, C] - binary or soft labels\n",
        "        \"\"\"\n",
        "        probs = torch.sigmoid(logits)  # [B, C]\n",
        "        ce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')  # [B, C]\n",
        "\n",
        "        pt = probs * targets + (1 - probs) * (1 - targets)  # p_t\n",
        "        focal_weight = (1 - pt) ** self.gamma               # (1 - pt)^γ\n",
        "\n",
        "        loss = focal_weight * ce_loss                       # focal weight 적용\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)  # [B, C]\n",
        "            loss = alpha_factor * loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zTgAvNcjFdzA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTgAvNcjFdzA",
        "outputId": "8a94398b-51c9-4f61-aaf7-eb1154a612e6"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "\n",
        "label_dist = Counter({0:456, 1:262, 2:84, 3:83})  # Finetune 분포\n",
        "\n",
        "# Crackle: (1 + Both), Wheeze: (2 + Both)\n",
        "n_crackle = label_dist[1] + label_dist[3]  # 262 + 83\n",
        "n_wheeze  = label_dist[2] + label_dist[3]  # 84 + 83\n",
        "n_total   = sum(label_dist.values())       # 885\n",
        "\n",
        "pos_weight = torch.tensor([\n",
        "    (n_total - n_crackle) / (n_crackle + 1e-6),\n",
        "    (n_total - n_wheeze) / (n_wheeze + 1e-6)\n",
        "], device=device)\n",
        "\n",
        "print(pos_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea74a2d",
      "metadata": {
        "id": "aea74a2d"
      },
      "source": [
        "## Linear Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nm8yaHDRZrT1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "Nm8yaHDRZrT1",
        "outputId": "c3eea671-d3e3-4eb3-a9c3-f47d1d2ae512"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8c5369e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "e8c5369e",
        "outputId": "a0a2ecb9-c2cd-4dbb-a981-8f210694face"
      },
      "outputs": [],
      "source": [
        "## Wandb 정의\n",
        "\n",
        "# import wandb\n",
        "finetune_project_name = f'SBW_aug(T.N)_LE_{args.target_sr}sr_{args.n_mels}mels{args.batch_size}bs_{get_timestamp()}'\n",
        "\n",
        "wandb.init(\n",
        "    project=\"SBW_ICBHI_MLS\",           # 프로젝트 이름\n",
        "    name=f\"{finetune_project_name}\", # 실험 이름\n",
        "    config={\n",
        "        \"epochs\": args.ft_epochs,\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"lr\": args.lr,\n",
        "        \"momentum\": args.momentum,\n",
        "        \"weight_decay\": args.weight_decay,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e2372b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "06e2372b",
        "outputId": "496ceb9b-53c2-4402-8845-cc42ae610591"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Model Load\n",
        "# 위에서부터 했다면\n",
        "load_ckpt_path = CHECKPOINT_PATH + f\"/{pretrain_project_name}_best_checkpoint.pth.tar\"\n",
        "# 중간부터 이어서 한다면\n",
        "# load_ckpt_path = CHECKPOINT_PATH + \"/SHS_aug(T.N)_PT_128bs_top15_0.5ld_2507110810_best_checkpoint.pth.tar\"\n",
        "\n",
        "# 저장 경로\n",
        "save_ckpt_path = CHECKPOINT_PATH+\"/LE_pth\"\n",
        "\n",
        "# 재현성을 위한 시드 재설정\n",
        "seed_everything(args.seed)\n",
        "\n",
        "# MoCo 모델 생성 및 체크포인트 로드\n",
        "model_eval = MoCo(\n",
        "    base_encoder=backbone_resnet,\n",
        "    dim_enc = args.out_dim,\n",
        "    dim_prj=args.dim_prj,\n",
        "    K=args.K,\n",
        "    m=args.momentum,\n",
        "    T=args.T,\n",
        "    top_k=args.top_k,\n",
        "    lambda_bce=args.lambda_bce\n",
        ")\n",
        "\n",
        "checkpoint = torch.load(load_ckpt_path, map_location=device)\n",
        "model_eval.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "# 사전 학습된 encoder 추출\n",
        "encoder = model_eval.encoder_q.to(device)\n",
        "\n",
        "# 2. Dataset 정의\n",
        "# Dataset 정의는 이미 되어있음 - test_loader\n",
        "\n",
        "# 3. Fine-tuning을 위한 분류 모델 정의 ( Data 개수 작으므로, encoder 파라미터 frozen )\n",
        "class FineTuningModel(nn.Module):\n",
        "    def __init__(self, encoder, out_dim=args.out_dim, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        # 마지막 FC layer를 제외한 encoder의 모든 레이어 freeze\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # 새로운 분류 헤드 추가\n",
        "        self.classifier = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "# 재현성을 위한 시드 재설정\n",
        "seed_everything(args.seed)\n",
        "\n",
        "# 4. 모델, 손실 함수, 옵티마이저 설정\n",
        "model = FineTuningModel(encoder, out_dim = args.out_dim).to(device)\n",
        "##############################\n",
        "\n",
        "# # Ablation(3-1) LE -> BCE Loss\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# # Ablation(3-2) LE -> Multi-label Focal Loss\n",
        "# criterion = MultiLabelFocalLoss(\n",
        "#     alpha=alpha_norm.to(device),  # 정규화된 값\n",
        "#     gamma=2.0,                    # hard label일 경우\n",
        "#     reduction='mean'\n",
        "# )\n",
        "\n",
        "############################\n",
        "optimizer = optim.AdamW(model.classifier.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "# optimizer = torch.optim.SGD(\n",
        "#     model.parameters(),\n",
        "#     lr=args.lr,\n",
        "#     momentum=0.9,\n",
        "#     weight_decay=args.weight_decay,\n",
        "#     nesterov=True\n",
        "# )\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=args.ft_epochs, eta_min=1e-6)  # Linear Evaluation에서 epochs는 다르게 적용\n",
        "\n",
        "# Best loss 초기화\n",
        "best_loss = float('inf')\n",
        "best_epoch = -1\n",
        "\n",
        "\n",
        "# 5. Linear Evaluation\n",
        "for epoch in range(args.ft_epochs):\n",
        "\n",
        "    # ===============================\n",
        "    # 1. Training\n",
        "    # ===============================\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_predictions = 0.0\n",
        "    correct_predictions = 0.0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    pbar = tqdm(finetune_loader, desc='Linear Evaluation')\n",
        "    for i, (cycle, labels, _) in enumerate(pbar):\n",
        "        # Forward pass\n",
        "        cycle = cycle.cuda(args.gpu)\n",
        "        labels = labels.cuda(args.gpu)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        output = model(cycle)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss 계산\n",
        "        total_loss += loss.item() # loss : -> float\n",
        "\n",
        "        # 예측값과 실제값 저장 ( Ablation(4-1) threshold ?? )\n",
        "        predicted = (torch.sigmoid(output) > 0.5).float()\n",
        "        all_preds.append(predicted.detach().cpu())\n",
        "        all_labels.append(labels.detach().cpu())\n",
        "        all_outputs.append(output.detach().cpu())\n",
        "\n",
        "    # train loss\n",
        "    train_loss = total_loss / len(finetune_loader)\n",
        "\n",
        "    # Concatenate\n",
        "    all_preds = torch.cat(all_preds, dim=0).numpy()    # shape: [N, 2]\n",
        "    all_labels = torch.cat(all_labels, dim=0).numpy()  # shape: [N, 2]\n",
        "    all_output = torch.cat(all_outputs, dim=0).numpy()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # =====================================\n",
        "    # 2-Edited. Multi-class 민감도/특이도 계산\n",
        "    # =====================================\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import wandb\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    def multilabel_to_multiclass(y):\n",
        "        # Crackle → 1, Wheeze → 2, Both → 3, None → 0\n",
        "        y = np.array(y)\n",
        "        return y[:, 0] + y[:, 1]*2\n",
        "\n",
        "    def evaluate_multiclass_confusion(y_true, y_pred, class_names=[\"Normal\", \"Wheeze\", \"Crackle\", \"Both\"]):\n",
        "        y_true_cls = multilabel_to_multiclass(y_true)\n",
        "        y_pred_cls = multilabel_to_multiclass(y_pred)\n",
        "\n",
        "        cm = confusion_matrix(y_true_cls, y_pred_cls, labels=[0, 1, 2, 3])\n",
        "\n",
        "        # N_n: 정상 → 정상\n",
        "        N_n = cm[0, 0]\n",
        "        N_total = cm[0].sum()\n",
        "\n",
        "        # 이상 클래스 정답 수: W, C, B\n",
        "        W_total = cm[1].sum()\n",
        "        C_total = cm[2].sum()\n",
        "        B_total = cm[3].sum()\n",
        "\n",
        "        # 각각의 정답 → 정확한 예측만 고려\n",
        "        W_w = cm[1, 1]\n",
        "        C_c = cm[2, 2]\n",
        "        B_b = cm[3, 3]\n",
        "\n",
        "        SP = N_n / (N_total + 1e-6) #spec\n",
        "        SE = (W_w + C_c + B_b) / (W_total + C_total + B_total + 1e-6) #sense\n",
        "\n",
        "        AS = (SP + SE) / 2\n",
        "        HS = 2 * SP * SE / (SP + SE + 1e-6)\n",
        "\n",
        "        return cm, SE, SP, y_true_cls, y_pred_cls\n",
        "\n",
        "    def log_multiclass_conf_matrix_wandb(cm, class_names, sens, spec, normalize, tag):\n",
        "        # Normalize (비율) 옵션\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "            fmt = '.2f'\n",
        "            title = \"Confusion Matrix (Normalized %)\"\n",
        "        else:\n",
        "            fmt = 'd'\n",
        "            title = \"Confusion Matrix (Raw Count)\"\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(7, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
        "\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('True')\n",
        "        ax.set_title(title)\n",
        "\n",
        "        icbhi_score = (sens + spec) / 2\n",
        "        # 우하단에 성능 출력\n",
        "        ax.text(\n",
        "            0.99, 0.15,\n",
        "            f\"Sensitivity: {sens*100:.2f}%\\nSpecificity: {spec*100:.2f}%\\nICBHI Score: {icbhi_score*100:.2f}%\",\n",
        "            ha='right', va='bottom',\n",
        "            transform=plt.gca().transAxes,\n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8)\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        # wandb.log({tag: wandb.Image(fig)})\n",
        "        # plt.close(fig)\n",
        "        return fig\n",
        "\n",
        "    # 1. 4-class Confusion Matrix 평가\n",
        "    class_names = [\"Normal\", \"Crackle\", \"Wheeze\", \"Both\"]\n",
        "    cm_4x4, finetune_train_sens, finetune_train_spec, y_true_cls, y_pred_cls = evaluate_multiclass_confusion(all_labels, all_preds, class_names)\n",
        "    finetune_icbhi_score = (finetune_train_sens + finetune_train_spec)/2\n",
        "\n",
        "    print(\"4-Class Confusion Matrix:\\n\", cm_4x4)\n",
        "    print(f\"Sensitivity: {finetune_train_sens:.4f}, Specificity: {finetune_train_spec:.4f}, ICBHI Score: {finetune_icbhi_score:.4f}\")\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 3. Validation\n",
        "    # ===============================\n",
        "    test_loss, test_labels, test_preds = validate(\n",
        "        model, test_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    precision = precision_score(test_labels, test_preds, average='macro')\n",
        "    recall = recall_score(test_labels, test_preds, average='macro')\n",
        "    f1 = f1_score(test_labels, test_preds, average='macro')\n",
        "\n",
        "    test_cm_4x4, test_sens, test_spec, test_y_true_cls, test_y_pred_cls = evaluate_multiclass_confusion(test_labels, test_preds)\n",
        "    test_icbhi_score = (test_sens+test_spec)/2\n",
        "\n",
        "    print(\"[Validation] Confusion Matrix:\\n\", test_cm_4x4)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"[VALIDATION] Sensitivity: {test_sens:.4f}, Specificity: {test_spec:.4f}, Avg ICBHI Score: {(test_sens+test_spec)/2:.4f}\")\n",
        "    print(\"##################################################\")\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 4. Confusion Matrix\n",
        "    # ===============================\n",
        "\n",
        "    # 2. Finetune Count Confusion Matrix 시각화\n",
        "    fig_finetune_raw = log_multiclass_conf_matrix_wandb(cm_4x4, class_names, finetune_train_sens, finetune_train_spec, normalize=False, tag=\"finetune_conf_matrix_raw\")\n",
        "    fig_finetune_norm = log_multiclass_conf_matrix_wandb(cm_4x4, class_names, finetune_train_sens, finetune_train_spec, normalize=True, tag=\"finetune_conf_matrix_norm\")\n",
        "\n",
        "    # 3. Test Confusion Matrix 시각화\n",
        "    fig_test_raw = log_multiclass_conf_matrix_wandb(test_cm_4x4, class_names, test_sens, test_spec, normalize=False, tag=\"test_conf_matrix_raw\")\n",
        "    fig_test_norm = log_multiclass_conf_matrix_wandb(test_cm_4x4, class_names, test_sens, test_spec, normalize=True, tag=\"test_conf_matrix_norm\")\n",
        "\n",
        "    # 4. log dictionary 생성\n",
        "    wandb_log_dict = {\n",
        "        \"finetune_conf_matrix_raw\": wandb.Image(fig_finetune_raw),\n",
        "        \"finetune_conf_matrix_norm\": wandb.Image(fig_finetune_norm),\n",
        "        \"test_conf_matrix_raw\": wandb.Image(fig_test_raw),\n",
        "        \"test_conf_matrix_norm\": wandb.Image(fig_test_norm)\n",
        "    }\n",
        "\n",
        "    # =====================================\n",
        "    # 5. Checkpoint (Every 50 epochs)\n",
        "    # =====================================\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        ckpt_path = save_ckpt_path + f\"{finetune_project_name}_{epoch:03d}.pth.tar\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, ckpt_path)\n",
        "        print(f\"💾 Saved checkpoint to {save_ckpt_path}\")\n",
        "\n",
        "    # ===============================\n",
        "    # 6. Save Best Checkpoint\n",
        "    # ===============================\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        best_epoch = epoch\n",
        "        best_ckpt_path = save_ckpt_path + f\"{finetune_project_name}_best.pth.tar\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'loss': best_loss\n",
        "        }, best_ckpt_path)\n",
        "        print(f\"=> Saved best checkpoint (epoch: {epoch}, loss: {best_loss:.4f})\")\n",
        "\n",
        "\n",
        "        # 🔹 Confusion Matrix Logging for Best\n",
        "        cm_best, sens_best, spec_best,_, _ = evaluate_multiclass_confusion(test_labels, test_preds, class_names)\n",
        "        fig_best_raw = log_multiclass_conf_matrix_wandb(cm_best, class_names, sens_best, spec_best, normalize=False, tag=\"best_test_conf_matrix_raw\")\n",
        "\n",
        "        fig_best_norm = log_multiclass_conf_matrix_wandb(cm_best, class_names, sens_best, spec_best, normalize=True, tag=\"best_test_conf_matrix_norm\")\n",
        "\n",
        "        wandb_log_dict.update({\n",
        "            \"best_test_conf_matrix_raw\": wandb.Image(fig_best_raw),\n",
        "            \"best_test_conf_matrix_norm\": wandb.Image(fig_best_norm)\n",
        "        })\n",
        "\n",
        "\n",
        "    if epoch == args.ft_epochs - 1:\n",
        "        # 🔸 Confusion Matrix Logging for Last Epoch\n",
        "        cm_last, sens_last, spec_last, _, _  = evaluate_multiclass_confusion(test_labels, test_preds, class_names)\n",
        "        fig_last_raw = log_multiclass_conf_matrix_wandb(cm_last, class_names, sens_last, spec_last, normalize=False, tag=\"last_test_conf_matrix_raw\")\n",
        "\n",
        "        fig_last_norm = log_multiclass_conf_matrix_wandb(cm_last, class_names, sens_last, spec_last, normalize=True, tag=\"last_test_conf_matrix_norm\")\n",
        "\n",
        "        wandb_log_dict.update({\n",
        "            \"last_test_conf_matrix_raw\": wandb.Image(fig_last_raw),\n",
        "            \"last_test_conf_matrix_norm\": wandb.Image(fig_last_norm)\n",
        "        })\n",
        "    # =====================================\n",
        "    # 7. Logging with wandb confusion matrix\n",
        "    # =====================================\n",
        "\n",
        "    # step 1. metrics\n",
        "    wandb.log({\n",
        "        # Train metrics\n",
        "        \"Finetune/epoch\": epoch,\n",
        "        \"Finetune/train_loss\": train_loss,\n",
        "        \"Finetune/test_loss\": test_loss,\n",
        "        \"Finetune/train_sens\": finetune_train_sens,\n",
        "        \"Finetune/train_spec\": finetune_train_spec,\n",
        "        \"Finetune/icbhi_score\": finetune_icbhi_score,\n",
        "\n",
        "        # Test metrics\n",
        "        \"Test/loss\": test_loss,\n",
        "        \"Test/sensitivity\": test_sens,\n",
        "        \"Test/specificity\": test_spec,\n",
        "        \"Test/icbhi_score\": test_icbhi_score\n",
        "    })\n",
        "\n",
        "    # step 2. Confusion matrix\n",
        "    wandb.log(wandb_log_dict)\n",
        "\n",
        "    plt.close(fig_finetune_raw)\n",
        "    plt.close(fig_finetune_norm)\n",
        "    plt.close(fig_test_raw)\n",
        "    plt.close(fig_test_norm)\n",
        "    if 'fig_best_raw' in locals(): plt.close(fig_best_raw)\n",
        "    if 'fig_best_norm' in locals(): plt.close(fig_best_norm)\n",
        "    if 'fig_last_raw' in locals(): plt.close(fig_last_raw)\n",
        "    if 'fig_last_norm' in locals(): plt.close(fig_last_norm)\n",
        "\n",
        "    # ===============================\n",
        "    # 8. Scheduler Step\n",
        "    # ===============================\n",
        "    scheduler.step()\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "boaz",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
